{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bertQA_50.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "x5arM3DfYyTO"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "da3d5e55b19e4900ac7f1da446378596": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5cb796be60944ceda7baea4f2a23db53",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_194b9bb0ec5545fea01731bc1b1e9f9f",
              "IPY_MODEL_66697f0c2b8a426383f150ca689b8786"
            ]
          }
        },
        "5cb796be60944ceda7baea4f2a23db53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "194b9bb0ec5545fea01731bc1b1e9f9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7b29dac400ef478f9ecb4d8490147254",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5546778fee044a4797544a9bfc8189b6"
          }
        },
        "66697f0c2b8a426383f150ca689b8786": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ed8b79f0b71b4f6ca4ba18c22746b43f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 2.77MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4f0c62899b014e7982941a968da2a45b"
          }
        },
        "7b29dac400ef478f9ecb4d8490147254": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5546778fee044a4797544a9bfc8189b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ed8b79f0b71b4f6ca4ba18c22746b43f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4f0c62899b014e7982941a968da2a45b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySXHdHsEUxNd",
        "colab_type": "code",
        "outputId": "e7f76560-fe7c-4471-9c88-cbb26628a3fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-hWS0v0sx7J",
        "colab_type": "code",
        "outputId": "7be92924-edae-40b7-b129-9ff1ca386da7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "import pickle\n",
        "import random\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "import pandas as pd\n",
        "from itertools import islice\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn.functional import softmax\n",
        "\n",
        "# Setting device on GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "print()\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "torch.manual_seed(1234)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "\n",
            "Tesla P100-PCIE-16GB\n",
            "Memory Usage:\n",
            "Allocated: 0.0 GB\n",
            "Cached:    0.0 GB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fafa8b835d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX3dEiY-MzxU",
        "colab_type": "code",
        "outputId": "37fb73a0-7a7e-4d9f-ef33-b3469ec2f527",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        }
      },
      "source": [
        "!pip install transformers\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.6.0)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.26)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.26 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.26)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.26->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.26->boto3->transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhPBExB2bjG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"drive/My Drive/fiqa/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBGukeI_cCPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from evaluate import *\n",
        "from utils import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aemAO1Hd6rnh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import AutoModelWithLMHead, AutoTokenizer, pipeline\n",
        "\n",
        "# model = AutoModelWithLMHead.from_pretrained(path+\"model/lm_model\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(path+\"model/lm_model\")\n",
        "\n",
        "# fill_mask = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n",
        "# fill_mask(\"In both cases, [MASK] arrangements are designed to make a large profit for the owner.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diWEgRaLGoyb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_cands(cands_path):\n",
        "\n",
        "    qid_ranked_docs = {}\n",
        "\n",
        "    with open(cands_path,'r') as f:\n",
        "        for line in f:\n",
        "            \n",
        "            line = line.strip().split('\\t')\n",
        "            qid = int(line[0])\n",
        "            doc_id = int(line[1])\n",
        "            rank = int(line[2])\n",
        "\n",
        "            if qid not in qid_ranked_docs:\n",
        "                # Create a list for each query to store the candidates\n",
        "                candidates = [0]*50\n",
        "                qid_ranked_docs[qid] = candidates\n",
        "            qid_ranked_docs[qid][rank-1] = doc_id\n",
        "\n",
        "    return qid_ranked_docs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9kxwJskHG1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_dataset(qid_rel, cands):\n",
        "    test_set = []\n",
        "\n",
        "    for qid, docid in qid_rel.items():\n",
        "        \n",
        "        for ques, cand in cands.items():\n",
        "            if 0 not in cand:\n",
        "                cand_ans = cand\n",
        "                if ques == qid:\n",
        "                    tmp = []\n",
        "                    tmp.append(qid)\n",
        "                    tmp.append(docid)\n",
        "                    tmp.append(cand_ans)\n",
        "                    test_set.append(tmp)\n",
        "\n",
        "    for row in test_set:\n",
        "        assert len(row[2]) == 50, \"Dataset size is incorrect!\"\n",
        "    \n",
        "    return test_set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iCcUYUvb6-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dictionary - key: qid, value: list of positive docid\n",
        "train_qid_rel = load_pickle(path + \"data/qid_rel_train.pickle\")\n",
        "test_qid_rel = load_pickle(path + \"data/qid_rel_test.pickle\")\n",
        "valid_qid_rel = load_pickle(path + \"data/qid_rel_valid.pickle\")\n",
        "\n",
        "# List of lists:\n",
        "# Each element is a list containing [qid, positive docid, negative docid]\n",
        "# train_set = load_pickle(path + 'new-data/data_75/train_set_75.pickle')\n",
        "# valid_set = load_pickle(path + 'new-data/data_75/valid_set_75.pickle')\n",
        "# train_set = load_pickle(path + 'new-data/data_50/train_set_50.pickle')\n",
        "# valid_set = load_pickle(path + 'new-data/data_50/valid_set_50.pickle')\n",
        "# train_set = load_pickle(path + 'new-data/data_25/train_set_25.pickle')\n",
        "# valid_set = load_pickle(path + 'new-data/data_25/valid_set_25.pickle')\n",
        "# train_set = load_pickle(path + 'new-data/data_10/train_set_10.pickle')\n",
        "# valid_set = load_pickle(path + 'new-data/data_10/valid_set_10.pickle')\n",
        "# train_set = load_pickle(path + 'new-data/train_set.pickle')\n",
        "# valid_set = load_pickle(path + 'new-data/valid_set.pickle')\n",
        "\n",
        "# List of lists:\n",
        "# Each element is a list contraining [qid, list of pos docid, list of candidate docid]\n",
        "# Contains candidates with all pos docids\n",
        "# test_set = load_pickle(path + 'data/data_50/test_set_50.pickle')\n",
        "# Contains candidates retrieved by BM25\n",
        "# May be missing pos docids in candidates\n",
        "# test_set_full = load_pickle(path + 'new-data/data_50/test_set_full_50.pickle')\n",
        "\n",
        "# Dictionary mapping docid and qid to raw text\n",
        "docid_to_text = load_pickle(path + 'data/docid_to_text.pickle')\n",
        "qid_to_text = load_pickle(path + 'data/qid_to_text.pickle')\n",
        "\n",
        "# train_cands = load_cands(path + 'data/cands_train_100.tsv')\n",
        "# valid_cands = load_cands(path + 'data/cands_valid_100.tsv')\n",
        "# test_cands = load_cands(path + 'data/cands_test_100.tsv')\n",
        "\n",
        "# train_cands = load_cands(path + 'data/cands_train_50.tsv')\n",
        "# valid_cands = load_cands(path + 'data/cands_valid_50.tsv')\n",
        "# test_cands = load_cands(path + 'data/cands_test_50.tsv')\n",
        "\n",
        "# train_set = load_pickle(path + 'data/train_set.pickle')\n",
        "# valid_set = load_pickle(path + 'data/valid_set.pickle')\n",
        "# train_set = create_dataset(train_qid_rel, train_cands)\n",
        "# valid_set = create_dataset(valid_qid_rel, valid_cands)\n",
        "# test_set = create_dataset(test_qid_rel, test_cands)\n",
        "\n",
        "\n",
        "train_set = load_pickle(path + 'data/train_set_50.pickle')\n",
        "valid_set = load_pickle(path + 'data/valid_set_50.pickle')\n",
        "test_set = load_pickle(path + 'data/test_set_50.pickle')\n",
        "# train_set = create_dataset(train_qid_rel, train_cands)\n",
        "# valid_set = create_dataset(valid_qid_rel, valid_cands)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6lI80kbTo-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save_pickle(path+'data/test_set_50.pickle', test_set)\n",
        "# save_pickle(path+'data/valid_set_50.pickle', valid_set)\n",
        "save_pickle(path+'data/train_set_50.pickle', train_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdWsesbTVMt0",
        "colab_type": "code",
        "outputId": "3f5b6412-bc27-43f0-e090-b42c7af2c057",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "print(\"Number of training samples: {}\".format(len(train_set)))\n",
        "print(\"Number of validation samples: {}\".format(len(valid_set)))\n",
        "print(\"Number of test samples: {}\".format(len(test_set)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training samples: 5676\n",
            "Number of validation samples: 631\n",
            "Number of test samples: 333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6npVgMDNewIW",
        "colab_type": "code",
        "outputId": "eed7c64c-8309-4879-a6b9-2baceed583c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "da3d5e55b19e4900ac7f1da446378596",
            "5cb796be60944ceda7baea4f2a23db53",
            "194b9bb0ec5545fea01731bc1b1e9f9f",
            "66697f0c2b8a426383f150ca689b8786",
            "7b29dac400ef478f9ecb4d8490147254",
            "5546778fee044a4797544a9bfc8189b6",
            "ed8b79f0b71b4f6ca4ba18c22746b43f",
            "4f0c62899b014e7982941a968da2a45b"
          ]
        }
      },
      "source": [
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-large-uncased', do_lower_case=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da3d5e55b19e4900ac7f1da446378596",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_kGUNI4XK5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    # Get the column with the higher probability\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRSvaM8BEvH8",
        "colab_type": "code",
        "outputId": "01fe8dfb-ffd0-426a-92ef-4397e75a7250",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# Example of the training set [qid, pos docid, neg docid]\n",
        "print(train_set[0])"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, [18850], [531578, 417981, 324911, 524879, 397608, 216077, 173212, 434846, 104464, 326261, 528838, 234436, 571062, 196374, 481692, 207449, 338700, 153377, 406418, 327002, 421301, 11538, 375748, 238271, 322893, 130631, 483385, 73427, 560087, 531442, 156554, 541809, 562777, 192843, 553328, 283505, 209224, 351672, 324513, 18850, 55200, 540395, 297841, 367754, 455984, 160340, 577284, 287474, 565935, 354716]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x5arM3DfYyTO"
      },
      "source": [
        "## **Pairwise**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1x8kg7BBwYG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_pairwise_input_data(dataset, max_seq_len):\n",
        "    pos_input_ids = []\n",
        "    neg_input_ids = []\n",
        "\n",
        "    pos_type_ids = []\n",
        "    neg_type_ids = []\n",
        "    \n",
        "    pos_masks = []\n",
        "    neg_masks = []\n",
        "\n",
        "    pos_labels = []\n",
        "    neg_labels = []\n",
        "\n",
        "    for i, seq in enumerate(tqdm(dataset)):\n",
        "        qid, ans_labels, cands = seq[0], seq[1], seq[2]\n",
        "\n",
        "        filtered_cands = list(set(cands)-set(ans_labels))\n",
        "\n",
        "        pos_docid = random.choice(ans_labels)\n",
        "\n",
        "        # Map question id to text\n",
        "        q_text = qid_to_text[qid]\n",
        "\n",
        "        for neg_docid in filtered_cands:\n",
        "\n",
        "            # Map the docid to text\n",
        "            pos_ans_text = docid_to_text[pos_docid]\n",
        "            neg_ans_text = docid_to_text[neg_docid]\n",
        "\n",
        "            pos_encoded_seq = tokenizer.encode_plus(q_text, pos_ans_text, \n",
        "                                                max_length=max_seq_len, \n",
        "                                                pad_to_max_length=True, \n",
        "                                                return_token_type_ids=True,\n",
        "                                                return_attention_mask = True)\n",
        "            \n",
        "            neg_encoded_seq = tokenizer.encode_plus(q_text, neg_ans_text, \n",
        "                                                max_length=max_seq_len, \n",
        "                                                pad_to_max_length=True, \n",
        "                                                return_token_type_ids=True,\n",
        "                                                return_attention_mask = True)\n",
        "\n",
        "            pos_input_id = pos_encoded_seq['input_ids']\n",
        "            pos_type_id = pos_encoded_seq['token_type_ids']\n",
        "            pos_mask = pos_encoded_seq['attention_mask']\n",
        "\n",
        "            neg_input_id = neg_encoded_seq['input_ids']\n",
        "            neg_type_id = neg_encoded_seq['token_type_ids']\n",
        "            neg_mask = neg_encoded_seq['attention_mask']\n",
        "\n",
        "            pos_input_ids.append(pos_input_id)\n",
        "            pos_type_ids.append(pos_type_id)\n",
        "            pos_masks.append(pos_mask)\n",
        "            pos_labels.append(1)\n",
        "\n",
        "            neg_input_ids.append(neg_input_id)\n",
        "            neg_type_ids.append(neg_type_id)\n",
        "            neg_masks.append(neg_mask)\n",
        "            neg_labels.append(0)\n",
        "\n",
        "    return pos_input_ids, pos_type_ids, pos_masks, pos_labels, neg_input_ids, neg_type_ids, neg_masks, neg_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcYbXyudDgQn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "855f4104-f7d6-4110-ea2d-52718dd29c30"
      },
      "source": [
        "train_pos_input, train_pos_type_id, train_pos_att_mask, train_pos_labels, \\\n",
        "train_neg_input, train_neg_type_id, train_neg_att_mask, train_neg_labels  = get_pairwise_input_data(train_set, 128)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5676/5676 [37:18<00:00,  2.54it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8j7FG2OHRlh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "329a68d0-2163-4cd0-e9a2-cd46cff692d1"
      },
      "source": [
        "valid_pos_input, valid_pos_type_id, valid_pos_att_mask, valid_pos_labels, \\\n",
        "valid_neg_input, valid_neg_type_id, valid_neg_att_mask, valid_neg_labels = get_pairwise_input_data(valid_set, 128)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 631/631 [04:01<00:00,  2.61it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHGawog0Z461",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_pickle(path+'pairwise-data/train_pos_labels_50.pickle', train_pos_labels)\n",
        "save_pickle(path+'pairwise-data/train_neg_labels_50.pickle', train_neg_labels)\n",
        "save_pickle(path+'pairwise-data/valid_pos_labels_50.pickle', valid_pos_labels)\n",
        "save_pickle(path+'pairwise-data/valid_neg_labels_50.pickle', valid_neg_labels)\n",
        "\n",
        "save_pickle(path+'pairwise-data/train_pos_input_128_50.pickle', train_pos_input)\n",
        "save_pickle(path+'pairwise-data/train_neg_input_128_50.pickle', train_neg_input)\n",
        "save_pickle(path+'pairwise-data/valid_pos_input_128_50.pickle', valid_pos_input)\n",
        "save_pickle(path+'pairwise-data/valid_neg_input_128_50.pickle', valid_neg_input)\n",
        "\n",
        "save_pickle(path+'pairwise-data/train_pos_type_id_128_50.pickle', train_pos_type_id)\n",
        "save_pickle(path+'pairwise-data/train_neg_type_id_128_50.pickle', train_neg_type_id)\n",
        "save_pickle(path+'pairwise-data/valid_pos_type_id_128_50.pickle', valid_pos_type_id)\n",
        "save_pickle(path+'pairwise-data/valid_neg_type_id_128_50.pickle', valid_neg_type_id)\n",
        "\n",
        "save_pickle(path+'pairwise-data/train_pos_mask_128_50.pickle', train_pos_att_mask)\n",
        "save_pickle(path+'pairwise-data/train_neg_mask_128_50.pickle', train_neg_att_mask)\n",
        "save_pickle(path+'pairwise-data/valid_pos_mask_128_50.pickle', valid_pos_att_mask)\n",
        "save_pickle(path+'pairwise-data/valid_neg_mask_128_50.pickle', valid_neg_att_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdFPxvp-ahTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_pos_labels = load_pickle(path+'pairwise-data/train_pos_labels_50.pickle')\n",
        "train_neg_labels = load_pickle(path+'pairwise-data/train_neg_labels_50.pickle')\n",
        "valid_pos_labels = load_pickle(path+'pairwise-data/valid_pos_labels_50.pickle')\n",
        "valid_neg_labels = load_pickle(path+'pairwise-data/valid_neg_labels_50.pickle')\n",
        "\n",
        "train_pos_input = load_pickle(path+'pairwise-data/train_pos_input_128_50.pickle')\n",
        "train_neg_input = load_pickle(path+'pairwise-data/train_neg_input_128_50.pickle')\n",
        "valid_pos_input = load_pickle(path+'pairwise-data/valid_pos_input_128_50.pickle')\n",
        "valid_neg_input = load_pickle(path+'pairwise-data/valid_neg_input_128_50.pickle')\n",
        "\n",
        "train_pos_type_id = load_pickle(path+'pairwise-data/train_pos_type_id_128_50.pickle')\n",
        "train_neg_type_id = load_pickle(path+'pairwise-data/train_neg_type_id_128_50.pickle')\n",
        "valid_pos_type_id = load_pickle(path+'pairwise-data/valid_pos_type_id_128_50.pickle')\n",
        "valid_neg_type_id = load_pickle(path+'pairwise-data/valid_neg_type_id_128_50.pickle')\n",
        "\n",
        "train_pos_mask = load_pickle(path+'pairwise-data/train_pos_mask_128_50.pickle')\n",
        "train_neg_mask = load_pickle(path+'pairwise-data/train_neg_mask_128_50.pickle')\n",
        "valid_pos_mask = load_pickle(path+'pairwise-data/valid_pos_mask_128_50.pickle')\n",
        "valid_neg_mask = load_pickle(path+'pairwise-data/valid_neg_mask_128_50.pickle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7q6wCjj3XdB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "88935056-f38d-4352-91c9-65b1ba727388"
      },
      "source": [
        "print(len(train_pos_input))\n",
        "print(len(valid_pos_input))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "277827\n",
            "30874\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEL7jdoecvbG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_pos_labels = train_pos_labels[:100]\n",
        "# train_neg_labels = train_neg_labels[:100]\n",
        "# train_pos_input = train_pos_input[:100]\n",
        "# train_neg_input = train_neg_input[:100]\n",
        "# train_pos_type_id = train_pos_type_id[:100]\n",
        "# train_neg_type_id = train_neg_type_id[:100]\n",
        "# train_pos_mask = train_pos_mask[:100]\n",
        "# train_neg_mask = train_neg_mask[:100]\n",
        "\n",
        "# valid_pos_labels = valid_pos_labels[:10]\n",
        "# valid_neg_labels = valid_neg_labels[:10]\n",
        "# valid_pos_input = valid_pos_input[:10]\n",
        "# valid_neg_input = valid_neg_input[:10]\n",
        "# valid_pos_type_id = valid_pos_type_id[:10]\n",
        "# valid_neg_type_id = valid_neg_type_id[:10]\n",
        "# valid_pos_mask = valid_pos_mask[:10]\n",
        "# valid_neg_mask = valid_neg_mask[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6c9oM8jauOc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert lists to PyTorch tensors\n",
        "train_pos_inputs = torch.tensor(train_pos_input)\n",
        "train_neg_inputs = torch.tensor(train_neg_input)\n",
        "valid_pos_inputs = torch.tensor(valid_pos_input)\n",
        "valid_neg_inputs = torch.tensor(valid_neg_input)\n",
        "\n",
        "train_pos_labels = torch.tensor(train_pos_labels)\n",
        "train_neg_labels = torch.tensor(train_neg_labels)\n",
        "valid_pos_labels = torch.tensor(valid_pos_labels)\n",
        "valid_neg_labels = torch.tensor(valid_neg_labels)\n",
        "\n",
        "train_pos_type_ids = torch.tensor(train_pos_type_id)\n",
        "train_neg_type_ids = torch.tensor(train_neg_type_id)\n",
        "valid_pos_type_ids = torch.tensor(valid_pos_type_id)\n",
        "valid_neg_type_ids = torch.tensor(valid_neg_type_id)\n",
        "\n",
        "train_pos_masks = torch.tensor(train_pos_mask)\n",
        "train_neg_masks = torch.tensor(train_neg_mask)\n",
        "valid_pos_masks = torch.tensor(valid_pos_mask)\n",
        "valid_neg_masks = torch.tensor(valid_neg_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3sBUeEuDau3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create DataLoaders to train the model in batches\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_pos_inputs, train_pos_type_ids, train_pos_masks, train_pos_labels, train_neg_inputs, train_neg_type_ids, train_neg_masks, train_neg_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(valid_pos_inputs, valid_pos_type_ids, valid_pos_masks, valid_pos_labels, valid_neg_inputs, valid_neg_type_ids, valid_neg_masks, valid_neg_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46AZOrGeawlT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pairwise_loss(pos_scores, neg_scores):\n",
        "    \"\"\"\n",
        "    Pairwise learning approach introduced in https://arxiv.org/pdf/1905.07588.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    cross_entropy_loss = -torch.log(pos_scores) - torch.log(1 - neg_scores)\n",
        "\n",
        "    margin = 1\n",
        "\n",
        "    hinge_loss = torch.max(torch.tensor(0, dtype=torch.float).to(device), margin - pos_scores + neg_scores)\n",
        "\n",
        "    loss = (0.5 * cross_entropy_loss + 0.5 * hinge_loss)\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ofMf9vDazyA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_pairwise(model, train_dataloader, optimizer, scheduler):\n",
        "\n",
        "    # Reset the loss and accuracy for each epoch\n",
        "    total_loss = 0\n",
        "    nb_train_steps = 0\n",
        "    train_accuracy = 0\n",
        "\n",
        "    # Set model in training mode\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
        "\n",
        "        # batch contains eight PyTorch tensors:\n",
        "        pos_input = batch[0].to(device)\n",
        "        pos_type_id = batch[1].to(device)\n",
        "        pos_mask = batch[2].to(device)\n",
        "        pos_labels = batch[3].to(device)\n",
        "\n",
        "        neg_input = batch[4].to(device)\n",
        "        neg_type_id = batch[5].to(device)\n",
        "        neg_mask = batch[6].to(device)\n",
        "        neg_labels = batch[7].to(device)\n",
        "\n",
        "        # Zero gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        # Compute predictinos for postive and negative QA pairs\n",
        "        pos_outputs = model(pos_input, token_type_ids=pos_type_id, attention_mask=pos_mask, labels=pos_labels)\n",
        "        neg_outputs = model(neg_input, token_type_ids=neg_type_id, attention_mask=neg_mask, labels=neg_labels)\n",
        "\n",
        "        # Get the logits from the model for positive and negative QA pairs\n",
        "        pos_logits = pos_outputs[1]\n",
        "        neg_logits = neg_outputs[1]\n",
        "\n",
        "        # Get the column of the relevant scores and apply activation function\n",
        "        pos_scores = softmax(pos_logits, dim=1)[:,1]\n",
        "        neg_scores = softmax(neg_logits, dim=1)[:,1]\n",
        "        \n",
        "        # Compute pairwise loss and get the mean of each batch\n",
        "        loss = pairwise_loss(pos_scores, neg_scores).mean()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        p_logits = pos_logits.detach().cpu().numpy()\n",
        "        p_labels = pos_labels.to('cpu').numpy()\n",
        "        n_logits = neg_logits.detach().cpu().numpy()\n",
        "        n_labels = neg_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for each batch\n",
        "        tmp_pos_accuracy = flat_accuracy(p_logits, p_labels)\n",
        "        tmp_neg_accuracy = flat_accuracy(n_logits, n_labels)\n",
        "\n",
        "        # Accumulate the total accuracy.\n",
        "        train_accuracy += tmp_pos_accuracy\n",
        "        train_accuracy += tmp_neg_accuracy\n",
        "        \n",
        "        # Track the number of batches (2 for pos and neg accuracies)\n",
        "        nb_train_steps += 2\n",
        "\n",
        "        # Accumulate the training loss over all of the batches\n",
        "        total_loss += loss.item()\n",
        "    \n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "\n",
        "    # Compute accuracy for each epoch\n",
        "    acc = train_accuracy/nb_train_steps\n",
        "\n",
        "    return avg_train_loss, acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wZBD9-BdFEz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate_pairwise(model, validation_dataloader):\n",
        "\n",
        "    # Set model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "    eval_accuracy = 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in tqdm(validation_dataloader):\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        pos_input, pos_type_id, pos_mask, pos_labels, neg_input, neg_type_id, neg_mask, neg_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():\n",
        "            # Compute predictinos for postive and negative QA pairs\n",
        "            pos_outputs = model(pos_input, token_type_ids=pos_type_id, attention_mask=pos_mask, labels=pos_labels)\n",
        "            neg_outputs = model(neg_input, token_type_ids=neg_type_id, attention_mask=neg_mask, labels=neg_labels)\n",
        "\n",
        "            # Get logits\n",
        "            pos_logits = pos_outputs[1]\n",
        "            neg_logits = neg_outputs[1]\n",
        "\n",
        "            # Apply activation function\n",
        "            pos_scores = softmax(pos_logits, dim=1)[:,1]\n",
        "            neg_scores = softmax(neg_logits, dim=1)[:,1]\n",
        "        \n",
        "        loss = pairwise_loss(pos_scores, neg_scores).mean()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        p_logits = pos_logits.detach().cpu().numpy()\n",
        "        p_labels = pos_labels.to('cpu').numpy()\n",
        "        n_logits = neg_logits.detach().cpu().numpy()\n",
        "        n_labels = neg_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_pos_accuracy = flat_accuracy(p_logits, p_labels)\n",
        "        tmp_neg_accuracy = flat_accuracy(n_logits, n_labels)\n",
        "\n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_pos_accuracy\n",
        "        eval_accuracy += tmp_neg_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 2\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(validation_dataloader)\n",
        "    acc = eval_accuracy/nb_eval_steps\n",
        "\n",
        "    return avg_loss, acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lxFYsHtADVla"
      },
      "source": [
        "## **Pointwise**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkOOxhr1KCeU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_input_data(dataset, max_seq_len):\n",
        "    input_ids = []\n",
        "    token_type_ids = []\n",
        "    att_masks = []\n",
        "    labels = []\n",
        "\n",
        "    for i, seq in enumerate(tqdm(dataset)):\n",
        "        qid, ans_labels, cands = seq[0], seq[1], seq[2]\n",
        "\n",
        "        # Map question id to text\n",
        "        q_text = qid_to_text[qid]\n",
        "\n",
        "        for docid in cands:\n",
        "\n",
        "            # Map the docid to text\n",
        "            ans_text = docid_to_text[docid]\n",
        "\n",
        "            encoded_seq = tokenizer.encode_plus(q_text, ans_text, \n",
        "                                                max_length=max_seq_len, \n",
        "                                                pad_to_max_length=True, \n",
        "                                                return_token_type_ids=True,\n",
        "                                                return_attention_mask = True)\n",
        "\n",
        "            input_id = encoded_seq['input_ids']\n",
        "            token_type_id = encoded_seq['token_type_ids']\n",
        "            att_mask = encoded_seq['attention_mask']\n",
        "\n",
        "            if docid in ans_labels:\n",
        "                label = 1\n",
        "            else:\n",
        "                label = 0\n",
        "\n",
        "            assert len(input_id) == max_seq_len, \"Input id dimension incorrect!\"\n",
        "            assert len(token_type_id) == max_seq_len, \"Token type id dimension incorrect!\"\n",
        "            assert len(att_mask) == max_seq_len, \"Attention mask dimension incorrect!\"\n",
        "\n",
        "            input_ids.append(input_id)\n",
        "            token_type_ids.append(token_type_id)\n",
        "            att_masks.append(att_mask)\n",
        "            labels.append(label)\n",
        "\n",
        "    return input_ids, token_type_ids, att_masks, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfTcRrN6J4m7",
        "colab_type": "code",
        "outputId": "b3823d28-0ece-45c5-91e6-e94fe12f3690",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "train_input, train_type_id, train_att_mask, train_label = get_input_data(train_set, 128)\n",
        "valid_input, valid_type_id, valid_att_mask, valid_label = get_input_data(valid_set, 128)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5676/5676 [18:33<00:00,  5.10it/s]\n",
            "100%|██████████| 631/631 [01:57<00:00,  5.38it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMBJFDBnXUr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_pickle(path+'/pointwise-data/train_input_128_lm.pickle', train_input)\n",
        "save_pickle(path+'/pointwise-data/train_type_id_128_lm.pickle', train_type_id)\n",
        "save_pickle(path+'/pointwise-data/train_mask_128_lm.pickle', train_att_mask)\n",
        "save_pickle(path+'/pointwise-data/train_labels_128_lm.pickle', train_label)\n",
        "\n",
        "save_pickle(path+'/pointwise-data/valid_input_128_lm.pickle', valid_input)\n",
        "save_pickle(path+'/pointwise-data/valid_type_id_128_lm.pickle', valid_type_id)\n",
        "save_pickle(path+'/pointwise-data/valid_mask_128_lm.pickle', valid_att_mask)\n",
        "save_pickle(path+'/pointwise-data/valid_labels_128_lm.pickle', valid_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6QF9C8NYSkw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_input = load_pickle(path+'/pointwise-data/train_input_128_lm.pickle')\n",
        "train_type_id = load_pickle(path+'/pointwise-data/train_type_id_128_lm.pickle')\n",
        "train_att_mask = load_pickle(path+'/pointwise-data/train_mask_128_lm.pickle')\n",
        "train_label = load_pickle(path+'/pointwise-data/train_labels_128_lm.pickle')\n",
        "\n",
        "valid_input = load_pickle(path+'/pointwise-data/valid_input_128_lm.pickle')\n",
        "valid_type_id = load_pickle(path+'/pointwise-data/valid_type_id_128_lm.pickle')\n",
        "valid_att_mask = load_pickle(path+'/pointwise-data/valid_mask_128_lm.pickle')\n",
        "valid_label = load_pickle(path+'/pointwise-data/valid_labels_128_lm.pickle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abSdoj111348",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_input = load_pickle(path+'/pointwise-data/train_input_128_50.pickle')\n",
        "train_type_id = load_pickle(path+'/pointwise-data/train_type_id_128_50.pickle')\n",
        "train_att_mask = load_pickle(path+'/pointwise-data/train_mask_128_50.pickle')\n",
        "train_label = load_pickle(path+'/pointwise-data/train_labels_128_50.pickle')\n",
        "\n",
        "valid_input = load_pickle(path+'/pointwise-data/valid_input_128_50.pickle')\n",
        "valid_type_id = load_pickle(path+'/pointwise-data/valid_type_id_128_50.pickle')\n",
        "valid_att_mask = load_pickle(path+'/pointwise-data/valid_mask_128_50.pickle')\n",
        "valid_label = load_pickle(path+'/pointwise-data/valid_labels_128_50.pickle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NIXDVVCUFMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def get_sequence_df(dataset):\n",
        "#     \"\"\"\n",
        "#     Converts training and validation data into a df with relevancy labels\n",
        "#     and map the qid and docid to text.\n",
        "    \n",
        "#     Returns data_df: df with columns qid, docid, label, question (text), answer (text)\n",
        "#     ---------------\n",
        "#     dataset: train or validation set in the form of list of lists\n",
        "#     \"\"\"\n",
        "#     # Load list into a dataframe\n",
        "#     df = pd.DataFrame(dataset)\n",
        "#     df = df.rename(columns={0: 'qid', 1: 'pos', 2:'neg'})\n",
        "#     # Construct new df with positive docids\n",
        "#     df_pos = df[['qid', 'pos']]\n",
        "#     df_pos = df_pos.rename(columns={'pos': 'docid'})\n",
        "#     # Add new column and assign positive label\n",
        "#     df_pos['label'] = df_pos.apply(lambda x: 1, axis=1)\n",
        "#     df_pos = df_pos.drop_duplicates()\n",
        "\n",
        "#     # Construct new df with negative docids\n",
        "#     df_neg = df[['qid', 'neg']]\n",
        "#     df_neg = df_neg.rename(columns={'neg': 'docid'})\n",
        "#     # Add new column and assign negative label\n",
        "#     df_neg['label'] = df_neg.apply(lambda x: 0, axis=1)\n",
        "\n",
        "#     # Concatenate the positive and negative df\n",
        "#     data_df = pd.concat([df_pos, df_neg]).sort_values(by=['qid'])\n",
        "\n",
        "#     # Map id to text\n",
        "#     data_df['question'] = data_df['qid'].apply(lambda x: qid_to_text[x])\n",
        "#     data_df['ans_cand'] = data_df['docid'].apply(lambda x: docid_to_text[x])\n",
        "\n",
        "#     return data_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEgEpLIAUNZK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Create train/validation data\n",
        "# trainset = get_sequence_df(train_set)\n",
        "# train_questions = trainset.question.values\n",
        "# train_answers = trainset.ans_cand.values\n",
        "# train_labels = trainset.label.values\n",
        "\n",
        "# train_input, train_type_id, train_att_mask = get_input(train_questions, train_answers, 128)\n",
        "\n",
        "# validset = get_sequence_df(valid_set)\n",
        "# valid_questions = validset.question.values\n",
        "# valid_answers = validset.ans_cand.values\n",
        "# valid_labels = validset.label.values\n",
        "\n",
        "# valid_input, valid_type_id, valid_att_mask = get_input(valid_questions, valid_answers, 128)\n",
        "\n",
        "# save_pickle(path+'/pointwise-data/train_labels_128_75.pickle', train_labels)\n",
        "# save_pickle(path+'/pointwise-data/valid_labels_128_75.pickle', valid_labels)\n",
        "\n",
        "# save_pickle(path+'/pointwise-data/train_input_128_75.pickle', train_input)\n",
        "# save_pickle(path+'/pointwise-data/valid_input_128_75.pickle', valid_input)\n",
        "\n",
        "# save_pickle(path+'/pointwise-data/train_type_id_128_75.pickle', train_type_id)\n",
        "# save_pickle(path+'/pointwise-data/valid_type_id_128_75.pickle', valid_att_mask)\n",
        "\n",
        "# save_pickle(path+'/pointwise-data/train_mask_128_75.pickle', train_att_mask)\n",
        "# save_pickle(path+'/pointwise-data/valid_mask_128_75.pickle', valid_att_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7ftXkM6Lu8d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Load train/validation data\n",
        "# train_label = load_pickle(path+'/pointwise-data/train_labels_128_50.pickle')\n",
        "# valid_label = load_pickle(path+'/pointwise-data/valid_labels_128_50.pickle')\n",
        "\n",
        "# # bert-base tokenizer\n",
        "# train_input = load_pickle(path+'/pointwise-data/train_input_128_50.pickle')\n",
        "# valid_input = load_pickle(path+'/pointwise-data/valid_input_128_50.pickle')\n",
        "# train_type_id = load_pickle(path+'/pointwise-data/train_type_id_128_50.pickle')\n",
        "# valid_type_id = load_pickle(path+'/pointwise-data/valid_type_id_128_50.pickle')\n",
        "# train_att_mask = load_pickle(path+'/pointwise-data/train_mask_128_50.pickle')\n",
        "# valid_att_mask = load_pickle(path+'/pointwise-data/valid_mask_128_50.pickle')\n",
        "\n",
        "# train_input = load_pickle(path+'/pointwise-data/train_input_256_10.pickle')\n",
        "# valid_input = load_pickle(path+'/pointwise-data/valid_input_256_10.pickle')\n",
        "# train_type_id = load_pickle(path+'/pointwise-data/train_type_id_256_10.pickle')\n",
        "# valid_type_id = load_pickle(path+'/pointwise-data/valid_type_id_256_10.pickle')\n",
        "# train_att_mask = load_pickle(path+'/pointwise-data/train_mask_256_10.pickle')\n",
        "# valid_att_mask = load_pickle(path+'/pointwise-data/valid_mask_256_10.pickle')\n",
        "\n",
        "\n",
        "# bert-large tokenizer\n",
        "# train_input = load_pickle(path+'/pointwise-data/train_input_256_large.pickle')\n",
        "# valid_input = load_pickle(path+'/pointwise-data/valid_input_256_large.pickle')\n",
        "# train_type_id = load_pickle(path+'/pointwise-data/train_type_id_256_large.pickle')\n",
        "# valid_type_id = load_pickle(path+'/pointwise-data/valid_type_id_256_large.pickle')\n",
        "# train_att_mask = load_pickle(path+'/pointwise-data/train_mask_256_large.pickle')\n",
        "# valid_att_mask = load_pickle(path+'/pointwise-data/valid_mask_256_large.pickle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKcl56lbMmQ0",
        "colab_type": "code",
        "outputId": "351c9859-05fa-4432-c61f-fbeef2b75c0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(len(train_input))\n",
        "print(len(valid_input))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "283800\n",
            "31550\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiCn2P_3MkN2",
        "colab_type": "code",
        "outputId": "72664a2a-8883-463b-fdd3-4bef33c25828",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Convert all inputs and labels into torch tensors\n",
        "train_inputs = torch.tensor(train_input)\n",
        "train_type_ids = torch.tensor(train_type_id)\n",
        "train_masks = torch.tensor(train_att_mask)\n",
        "train_labels = torch.tensor(train_label)\n",
        "\n",
        "validation_inputs = torch.tensor(valid_input)\n",
        "validation_type_ids = torch.tensor(valid_type_id)\n",
        "validation_masks = torch.tensor(valid_att_mask)\n",
        "validation_labels = torch.tensor(valid_label)\n",
        "\n",
        "# Create DataLoader to train in bacthes\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_type_ids, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_type_ids, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
        "\n",
        "print(\"Size of the DataLoader for the training set: {}\".format(len(train_dataloader)))\n",
        "print(\"Size of the DataLoader for the validation set: {}\".format(len(validation_dataloader)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of the DataLoader for the training set: 8869\n",
            "Size of the DataLoader for the validation set: 986\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KKtZ8j6Mzw8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_dataloader, optimizer, scheduler):\n",
        "\n",
        "    # Reset the total loss each epoch\n",
        "    total_loss = 0\n",
        "    train_accuracy = 0\n",
        "    # Track the number of batches\n",
        "    num_steps = 0\n",
        "\n",
        "    # Set model in train mode\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
        "\n",
        "        # batch contains four PyTorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: token_type_ids\n",
        "        #   [2]: attention masks\n",
        "        #   [3]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_token_type_ids = batch[1].to(device)\n",
        "        b_input_mask = batch[2].to(device)\n",
        "        b_labels = batch[3].to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Forward pass\n",
        "        # The model will return the loss and the logits\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids = b_token_type_ids, \n",
        "                    attention_mask = b_input_mask, \n",
        "                    labels = b_labels)\n",
        "\n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch\n",
        "        tmp_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        train_accuracy += tmp_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        num_steps += 1\n",
        "\n",
        "        # Accumulate the training loss over all of the batches\n",
        "        total_loss += loss.item()\n",
        "    \n",
        "        # Perform a backward pass to calculate the gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "\n",
        "    acc = train_accuracy/num_steps\n",
        "\n",
        "    return avg_train_loss, acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN-iWQHQM24D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(model, validation_dataloader):\n",
        "\n",
        "    # Evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0\n",
        "    eval_accuracy = 0\n",
        "    nb_eval_steps = 0\n",
        "\n",
        "    # For each batch of the validation data\n",
        "    for batch in tqdm(validation_dataloader):\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from the dataloader\n",
        "        b_input_ids, b_token_type_ids, b_input_masks, b_labels = batch\n",
        "        \n",
        "        # Don't to compute or store gradients\n",
        "        with torch.no_grad():        \n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids = b_token_type_ids, \n",
        "                            attention_mask = b_input_masks,\n",
        "                            labels= b_labels)\n",
        "        \n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    acc = eval_accuracy/nb_eval_steps\n",
        "    avg_loss = total_loss / len(validation_dataloader)\n",
        "\n",
        "    return avg_loss, acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "viv1NrZkLr8K"
      },
      "source": [
        "## **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An_zn-HiYolL",
        "colab_type": "code",
        "outputId": "7904f296-f7a5-4a9f-96ea-d3b7e679ea07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top\n",
        "\n",
        "# model_path = path + \"model/fin_model\"\n",
        "model_path = path + \"model/lm_model\"\n",
        "model = BertForSequenceClassification.from_pretrained(model_path, cache_dir=None, num_labels=2)\n",
        "\n",
        "# model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", cache_dir=None, num_labels=2)\n",
        "# model = BertForSequenceClassification.from_pretrained(\"bert-large-uncased\", cache_dir=None, num_labels=2)\n",
        "model.to(device)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(28989, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut7wlYPSlZcM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = AdamW(model.parameters(), lr = 3e-6, weight_decay=0.01)\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 3\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 10000,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2fZJST0_L2Q7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "eff94a12-92c5-446c-8235-bbe7dc357e87"
      },
      "source": [
        "# Lowest validation lost\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    train_loss_values = []\n",
        "    train_acc_values = []\n",
        "\n",
        "    valid_loss_values = []\n",
        "    valid_acc_values = []\n",
        "\n",
        "    # Evaluate training loss\n",
        "    train_loss, train_acc = train(model, train_dataloader, optimizer, scheduler)\n",
        "    # train_loss, train_acc = train_pairwise(model, train_dataloader, optimizer, scheduler)\n",
        "    train_loss_values.append(train_loss)\n",
        "    train_acc_values.append(train_acc)\n",
        "\n",
        "    # Evaluate validation loss\n",
        "    valid_loss, valid_acc = validate(model, validation_dataloader)\n",
        "    # valid_loss, valid_acc = validate_pairwise(model, validation_dataloader)\n",
        "    valid_loss_values.append(valid_loss)\n",
        "    valid_acc_values.append(valid_acc)\n",
        "    \n",
        "    # At each epoch, if the validation loss is the best\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "    torch.save(model.state_dict(), path + 'model/' + str(epoch+1)+'_pointwise_lm_128_32_3e6.pt')\n",
        "\n",
        "    print(\"\\n\\n Epoch {}:\".format(epoch+1))\n",
        "    print(\"\\t Train Loss: {} | Train Accuracy: {}%\".format(round(train_loss, 3), round(train_acc*100, 2)))\n",
        "    print(\"\\t Validation Loss: {} | Validation Accuracy: {}%\\n\".format(round(valid_loss, 3), round(valid_acc*100, 2)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  1%|          | 110/8869 [00:44<59:39,  2.45it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KHBg5FXKOYHU"
      },
      "source": [
        "## **Evalulation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCGbsz18-Urm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_rank(model, test_set, qid_rel, max_seq_len):\n",
        "    \"\"\"\n",
        "    Returns a dictionary - key: qid, value: list of ranked candidates\n",
        "    -------------------\n",
        "    model - PyTorch model\n",
        "    test_set - List of lists:\n",
        "            Each element is a list contraining \n",
        "            [qid, list of pos docid, list of candidate docid]\n",
        "    qid_rel: Dictionary\n",
        "            key: qid, value: list of relevant answer id\n",
        "    max_seq_len: int\n",
        "            Maximum sequence length\n",
        "    \"\"\"\n",
        "\n",
        "    # Initiate empty dictionary\n",
        "    qid_pred_rank = {}\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # For each element in the test set\n",
        "    for i, seq in enumerate(tqdm(test_set)):\n",
        "        \n",
        "        # question id, list of rel answers, list of candidates\n",
        "        qid, label, cands = seq[0], seq[1], seq[2]\n",
        "\n",
        "        # Map question id to text\n",
        "        q_text = qid_to_text[qid]\n",
        "\n",
        "        # Convert list to numpy array\n",
        "        cands_id = np.array(cands)\n",
        "\n",
        "        # Empty list for the probability scores of relevancy\n",
        "        scores = []\n",
        "\n",
        "        # For each answer in the candidates\n",
        "        for docid in cands:\n",
        "\n",
        "            # Map the docid to text\n",
        "            ans_text = docid_to_text[docid]\n",
        "\n",
        "            # Create inputs for the model\n",
        "            encoded_seq = tokenizer.encode_plus(q_text, ans_text, \n",
        "                                            max_length=max_seq_len, \n",
        "                                            pad_to_max_length=True, \n",
        "                                            return_token_type_ids=True,\n",
        "                                            return_attention_mask = True)\n",
        "\n",
        "            # Numericalized, padded, clipped seq with special tokens\n",
        "            input_ids = torch.tensor([encoded_seq['input_ids']]).to(device)\n",
        "            # Specify question seq and answer seq\n",
        "            token_type_ids = torch.tensor([encoded_seq['token_type_ids']]).to(device)\n",
        "            # Sepecify which position is part of the seq which is padded\n",
        "            att_mask = torch.tensor([encoded_seq['attention_mask']]).to(device)\n",
        "\n",
        "            # Don't calculate gradients\n",
        "            with torch.no_grad():\n",
        "            # Forward pass, calculate logit predictions for each QA pair\n",
        "                outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=att_mask)\n",
        "\n",
        "            # Get the predictions\n",
        "            logits = outputs[0]\n",
        "\n",
        "            # Apply activation function\n",
        "            pred = softmax(logits, dim=1)\n",
        "            # pred = torch.sigmoid(logits)\n",
        "\n",
        "            # Move logits and labels to CPU\n",
        "            pred = pred.detach().cpu().numpy()\n",
        "\n",
        "            # Append relevant scores to list (where label = 1)\n",
        "            scores.append(pred[:,1][0])\n",
        "\n",
        "        # print(scores)\n",
        "\n",
        "        # Get the indices of the sorted similarity scores\n",
        "        sorted_index = np.argsort(scores)[::-1]\n",
        "\n",
        "        # Get the list of docid from the sorted indices\n",
        "        ranked_ans = cands_id[sorted_index]\n",
        "\n",
        "        # Dict - key: qid, value: ranked list of docids\n",
        "        qid_pred_rank[qid] = ranked_ans\n",
        "\n",
        "    return qid_pred_rank"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOR-OM-qbb3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "toy_test_label = dict(itertools.islice(test_qid_rel.items(), 3))\n",
        "toy_test = test_set[:3]\n",
        "# toy_test = [[14, [398960], [84963, 14255, 398960]],\n",
        "#             [68, [19183], [107584, 562777, 19183]],\n",
        "#             [70, [327002], [107584, 327002, 19183]]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGa0KvT4cbbo",
        "colab_type": "code",
        "outputId": "16f0bfb8-83ee-4797-a257-a4d183f8fbc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model.load_state_dict(torch.load(path+'model/1_pairwisewise50_128_32_3e6_05.pt'))\n",
        "\n",
        "qid_pred_rank = get_rank(model, test_set, test_qid_rel, max_seq_len=128)\n",
        "# qid_pred_rank = get_rank(model, toy_test, toy_test_label, max_seq_len=128)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 333/333 [05:29<00:00,  1.01it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTc2SQVhuv2Q",
        "colab_type": "code",
        "outputId": "bc449914-d1c9-451e-dcbb-a7cd27ba35c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "k = 10\n",
        "\n",
        "num_q = len(test_set)\n",
        "\n",
        "MRR, average_ndcg, precision, rank_pos = evaluate(qid_pred_rank, test_qid_rel, k)\n",
        "# MRR, average_ndcg, precision, rank_pos = evaluate(qid_pred_rank, toy_test_label, k)\n",
        "\n",
        "print(\"\\n\\nAverage nDCG@{} for {} queries: {}\\n\".format(k, num_q, average_ndcg))\n",
        "\n",
        "print(\"MRR@{} for {} queries: {}\\n\".format(k, num_q, MRR))\n",
        "\n",
        "print(\"Average Precision@{}: {}\".format(1, precision))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Average nDCG@10 for 333 queries: 0.40536723247075324\n",
            "\n",
            "MRR@10 for 333 queries: 0.3471876638543306\n",
            "\n",
            "Average Precision@1: 0.2702702702702703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcfbXb5eBcX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_pickle(path+'rank/rank_1_pointwise50_128_32_3e6.pickle', qid_pred_rank)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}