{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy_of_bertQA.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "5YEbteNsrcM6"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySXHdHsEUxNd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-hWS0v0sx7J",
        "colab_type": "code",
        "outputId": "1ac09936-1ed7-49fa-8ed2-093583326025",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        }
      },
      "source": [
        "import pickle\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "import pandas as pd\n",
        "from itertools import islice\n",
        "import numpy as np\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import random\n",
        "!pip install transformers\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, BertConfig\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers import AdamW\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Setting device on GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "print()\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "    print('Memory Usage:')\n",
        "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Set the random seed manually for reproducibility.\n",
        "torch.manual_seed(1234)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.5.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.12)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.12 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.12)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.12->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.12->boto3->transformers) (2.8.1)\n",
            "Using device: cuda\n",
            "\n",
            "Tesla P100-PCIE-16GB\n",
            "Memory Usage:\n",
            "Allocated: 0.0 GB\n",
            "Cached:    0.0 GB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f6dfda399f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhPBExB2bjG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = \"drive/My Drive/FiQA/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBGukeI_cCPy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from evaluate import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbIIxiVhU1VL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def take(n, iterable):\n",
        "    \"Return first n items of the iterable as a list\"\n",
        "    return list(islice(iterable, n))\n",
        "\n",
        "def remove_empty(test_set):\n",
        "    for index, row in enumerate(test_set):\n",
        "        for doc in row[1]:\n",
        "            if doc in empty_docs:\n",
        "                del test_set[index]\n",
        "    return test_set\n",
        "\n",
        "def load_pickle(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def save_pickle(path, data):\n",
        "    with open(path, 'wb') as handle:\n",
        "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def pad_seq(seq, max_seq_len):\n",
        "    # Pad each seq to be the same length to process in batch.\n",
        "    # pad_token = 0\n",
        "    if len(seq) >= max_seq_len:\n",
        "        seq = seq[:max_seq_len]\n",
        "    else:\n",
        "        seq += [0]*(max_seq_len - len(seq))\n",
        "    return seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iCcUYUvb6-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dict mapping of token to idx\n",
        "vocab = load_pickle(path + 'vocab_full.pickle')\n",
        "# dict mapping of docid to doc text\n",
        "docid_to_text = load_pickle(path + 'label_ans.pickle')\n",
        "\n",
        "# dict mapping of qid to question text\n",
        "qid_to_text = load_pickle(path + 'qid_text.pickle')\n",
        "\n",
        "train_qid_rel = load_pickle(path + \"qid_rel_train.pickle\")\n",
        "test_qid_rel = load_pickle(path + \"qid_rel_test.pickle\")\n",
        "valid_qid_rel = load_pickle(path + \"qid_rel_valid.pickle\")\n",
        "\n",
        "train_set = load_pickle(path + 'data/data_train_50.pickle')\n",
        "valid_set = load_pickle(path + 'data/data_valid_50.pickle')\n",
        "\n",
        "test_set = load_pickle(path + 'data/data_test_500_rel.pickle')\n",
        "test_set_full = load_pickle(path + 'data/data_test_500.pickle')\n",
        "\n",
        "empty_docs = load_pickle(path+'empty_docs.pickle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9_sM2Lrb794",
        "colab_type": "code",
        "outputId": "aff0c111-3f62-45a4-9ac7-310dfdd4a972",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "train_set = [x for x in train_set if x[1] not in empty_docs]\n",
        "valid_set = [x for x in valid_set if x[1] not in empty_docs]\n",
        "\n",
        "test_set = remove_empty(test_set)\n",
        "test_set_full = remove_empty(test_set_full)\n",
        "\n",
        "print(\"Number of training samples: {}\".format(len(train_set)))\n",
        "print(\"Number of validation samples: {}\".format(len(valid_set)))\n",
        "print(\"Number of test samples: {}\".format(len(test_set)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training samples: 283707\n",
            "Number of validation samples: 31582\n",
            "Number of test samples: 330\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6npVgMDNewIW",
        "colab_type": "code",
        "outputId": "3ef29e27-9821-41ce-8b13-9136af5d67bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXwjgq4zuPzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_to_ans = load_pickle(path+\"data-bert/label_to_ans.pickle\")\n",
        "qid_to_text = load_pickle(path+\"data-bert/qid_to_text.pickle\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZERfpbehdwj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_question_token(q_tokens):\n",
        "    c = [\"[CLS]\"]\n",
        "    s = [\"[SEP]\"]\n",
        "    q_tokens = c + q_tokens\n",
        "    q_tokens = q_tokens + s\n",
        "\n",
        "    return q_tokens\n",
        "\n",
        "def add_ans_token(a_tokens):\n",
        "    s = [\"[SEP]\"]\n",
        "    a_tokens = a_tokens + s\n",
        "\n",
        "    return a_tokens\n",
        "\n",
        "def clip(lst):\n",
        "    max_seq_len = 512\n",
        "    if len(lst) > max_seq_len:\n",
        "        lst = lst[:max_seq_len]\n",
        "    else:\n",
        "        lst = lst\n",
        "    \n",
        "    return lst\n",
        "\n",
        "def get_input_ids(sequences, max_seq_len):\n",
        "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "    input_ids = []\n",
        "\n",
        "    for seq in sequences:\n",
        "        # `encode` will:\n",
        "        #   (1) Tokenize the sentence.\n",
        "        #   (2) Map tokens to their IDs.\n",
        "        encoded_seq = tokenizer.convert_tokens_to_ids(seq)\n",
        "        \n",
        "        # Add the encoded sentence to the list.\n",
        "        input_ids.append(encoded_seq)\n",
        "\n",
        "    input_ids = pad_sequences(input_ids, maxlen=max_seq_len, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "    return input_ids\n",
        "\n",
        "def get_att_mask(input_ids):\n",
        "    # Create attention masks\n",
        "    attention_masks = []\n",
        "\n",
        "    # For each sentence...\n",
        "    for sent in input_ids:\n",
        "        \n",
        "        # Create the attention mask.\n",
        "        #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "        #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "        att_mask = [int(token_id > 0) for token_id in sent]\n",
        "        \n",
        "        # Store the attention mask for this sentence.\n",
        "        attention_masks.append(att_mask)\n",
        "\n",
        "    return attention_masks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcnub5RAVxJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_sequence_df(dataset):\n",
        "    df = pd.DataFrame(dataset)\n",
        "    df = df.rename(columns={0: 'qid', 1: 'pos', 2:'neg'})\n",
        "    df_pos = df[['qid', 'pos']]\n",
        "    df_pos = df_pos.rename(columns={'pos': 'docid'})\n",
        "    df_pos['label'] = df_pos.apply(lambda x: 1, axis=1)\n",
        "    df_pos = df_pos.drop_duplicates()\n",
        "\n",
        "    df_neg = df[['qid', 'neg']]\n",
        "    df_neg = df_neg.rename(columns={'neg': 'docid'})\n",
        "    df_neg['label'] = df_neg.apply(lambda x: 0, axis=1)\n",
        "    data_df = pd.concat([df_pos, df_neg]).sort_values(by=['qid'])\n",
        "\n",
        "    data_df['question'] = data_df['qid'].apply(lambda x: qid_to_text[x])\n",
        "    data_df['ans_cand'] = data_df['docid'].apply(lambda x: label_to_ans[x])\n",
        "    data_df['ques_token'] = data_df['question'].apply(lambda x: add_question_token(x))\n",
        "    data_df['ans_cand'] = data_df['ans_cand'].apply(lambda x: add_ans_token(x))\n",
        "\n",
        "    data_df = data_df[['qid', 'docid', 'label', 'ans_cand','ques_token']]\n",
        "    data_df['seq'] = data_df['ques_token'] + data_df['ans_cand']\n",
        "\n",
        "    data_df['seq_clipped'] = data_df['seq'].apply(clip)\n",
        "    # train['len'] = train['seq_clipped'].apply(lambda x: len(x))\n",
        "\n",
        "    return data_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gd6H-9xlrS66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_pairwise_sequence_df(dataset):\n",
        "    df = pd.DataFrame(dataset)\n",
        "    df = df.rename(columns={0: 'qid', 1: 'pos_id', 2:'neg_id'})\n",
        "    df['pos_label'] = df.apply(lambda x: 1, axis=1)\n",
        "    df['neg_label'] = df.apply(lambda x: 0, axis=1)\n",
        "\n",
        "    df['question'] = df['qid'].apply(lambda x: qid_to_text[x])\n",
        "    df['pos_ans'] = df['pos_id'].apply(lambda x: label_to_ans[x])\n",
        "    df['neg_ans'] = df['neg_id'].apply(lambda x: label_to_ans[x])\n",
        "\n",
        "    df['ques_token'] = df['question'].apply(lambda x: add_question_token(x))\n",
        "    df['pos_ans'] = df['pos_ans'].apply(lambda x: add_ans_token(x))\n",
        "    df['neg_ans'] = df['neg_ans'].apply(lambda x: add_ans_token(x))\n",
        "\n",
        "    df = df[['qid', 'pos_id', 'neg_id', 'pos_label', 'neg_label', 'pos_ans', 'neg_ans', 'ques_token']]\n",
        "    df['pos_seq'] = df['ques_token'] + df['pos_ans']\n",
        "    df['neg_seq'] = df['ques_token'] + df['neg_ans']\n",
        "\n",
        "    df['pos_seq_clipped'] = df['pos_seq'].apply(clip)\n",
        "    df['neg_seq_clipped'] = df['neg_seq'].apply(clip)\n",
        "\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YEbteNsrcM6",
        "colab_type": "text"
      },
      "source": [
        "## **Pairwise**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjB1w2ZjrXAb",
        "colab_type": "code",
        "outputId": "10290662-0b2a-4bc2-fc7c-fd1cf77672fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "trainset = get_pairwise_sequence_df(train_set)\n",
        "validset = get_pairwise_sequence_df(valid_set)\n",
        "\n",
        "# Get the lists of sentences and their labels.\n",
        "train_pos_seq = trainset.pos_seq_clipped.values\n",
        "train_neg_seq = trainset.neg_seq_clipped.values\n",
        "train_pos_labels = trainset.pos_label.values\n",
        "train_neg_labels = trainset.neg_label.values\n",
        "\n",
        "valid_pos_seq = validset.pos_seq_clipped.values\n",
        "valid_neg_seq = validset.neg_seq_clipped.values\n",
        "valid_pos_labels = validset.pos_label.values\n",
        "valid_neg_labels = validset.neg_label.values\n",
        "\n",
        "print(len(train_pos_seq))\n",
        "print(len(valid_pos_seq))\n",
        "\n",
        "# train_pos_seq = train_pos_seq[:300]\n",
        "# train_neg_seq = train_neg_seq[:300]\n",
        "# train_pos_labels = train_pos_labels[:300]\n",
        "# train_neg_labels = train_neg_labels[:300]\n",
        "\n",
        "# valid_pos_seq = valid_pos_seq[:30]\n",
        "# valid_neg_seq = valid_neg_seq[:30]\n",
        "# valid_pos_labels = valid_pos_labels[:30]\n",
        "# valid_neg_labels = valid_neg_labels[:30]\n",
        "\n",
        "max_seq_len = 512\n",
        "\n",
        "train_pos_input = get_input_ids(train_pos_seq, max_seq_len)\n",
        "train_neg_input = get_input_ids(train_neg_seq, max_seq_len)\n",
        "valid_pos_input = get_input_ids(valid_pos_seq, max_seq_len)\n",
        "valid_neg_input = get_input_ids(valid_neg_seq, max_seq_len)\n",
        "\n",
        "train_pos_mask = get_att_mask(train_pos_input)\n",
        "train_neg_mask = get_att_mask(train_neg_input)\n",
        "valid_pos_mask = get_att_mask(valid_pos_input)\n",
        "valid_neg_mask = get_att_mask(valid_neg_input)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "283707\n",
            "31582\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNabMgS5-xtx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save_pickle(path+'/data-bert/train_pos_labels.pickle', train_pos_labels)\n",
        "# save_pickle(path+'/data-bert/train_neg_labels.pickle', train_neg_labels)\n",
        "# save_pickle(path+'/data-bert/valid_pos_labels.pickle', valid_pos_labels)\n",
        "# save_pickle(path+'/data-bert/valid_neg_labels.pickle', valid_neg_labels)\n",
        "\n",
        "save_pickle(path+'/data-bert/train_pos_input_512.pickle', train_pos_input)\n",
        "save_pickle(path+'/data-bert/train_neg_input_512.pickle', train_neg_input)\n",
        "save_pickle(path+'/data-bert/valid_pos_input_512.pickle', valid_pos_input)\n",
        "save_pickle(path+'/data-bert/valid_neg_input_512.pickle', valid_neg_input)\n",
        "\n",
        "save_pickle(path+'/data-bert/train_pos_mask_512.pickle', train_pos_mask)\n",
        "save_pickle(path+'/data-bert/train_neg_mask_512.pickle', train_neg_mask)\n",
        "save_pickle(path+'/data-bert/valid_pos_mask_512.pickle', valid_pos_mask)\n",
        "save_pickle(path+'/data-bert/valid_neg_mask_512.pickle', valid_neg_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkpbuKiD_jlx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_pos_labels = load_pickle(path+'/data-bert/train_pos_labels.pickle')\n",
        "train_neg_labels = load_pickle(path+'/data-bert/train_neg_labels.pickle')\n",
        "valid_pos_labels = load_pickle(path+'/data-bert/valid_pos_labels.pickle')\n",
        "valid_neg_labels = load_pickle(path+'/data-bert/valid_neg_labels.pickle')\n",
        "\n",
        "train_pos_input = load_pickle(path+'/data-bert/train_pos_input_512.pickle')\n",
        "train_neg_input = load_pickle(path+'/data-bert/train_neg_input_512.pickle')\n",
        "valid_pos_input = load_pickle(path+'/data-bert/valid_pos_input_512.pickle')\n",
        "valid_neg_input = load_pickle(path+'/data-bert/valid_neg_input_512.pickle')\n",
        "\n",
        "train_pos_mask = load_pickle(path+'/data-bert/train_pos_mask_512.pickle')\n",
        "train_neg_mask = load_pickle(path+'/data-bert/train_neg_mask_512.pickle')\n",
        "valid_pos_mask = load_pickle(path+'/data-bert/valid_pos_mask_512.pickle')\n",
        "valid_neg_mask = load_pickle(path+'/data-bert/valid_neg_mask_512.pickle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1vGWJhkrkkF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_pos_inputs = torch.tensor(train_pos_input)\n",
        "train_neg_inputs = torch.tensor(train_neg_input)\n",
        "valid_pos_inputs = torch.tensor(valid_pos_input)\n",
        "valid_neg_inputs = torch.tensor(valid_neg_input)\n",
        "\n",
        "train_pos_labels = torch.tensor(train_pos_labels)\n",
        "train_neg_labels = torch.tensor(train_neg_labels)\n",
        "valid_pos_labels = torch.tensor(valid_pos_labels)\n",
        "valid_neg_labels = torch.tensor(valid_neg_labels)\n",
        "\n",
        "train_pos_masks = torch.tensor(train_pos_mask)\n",
        "train_neg_masks = torch.tensor(train_neg_mask)\n",
        "valid_pos_masks = torch.tensor(valid_pos_mask)\n",
        "valid_neg_masks = torch.tensor(valid_neg_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wr11jao6rmmJ",
        "colab_type": "code",
        "outputId": "8a8a9e7c-a64f-4b29-ca87-895866f2623a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(len(train_pos_inputs))\n",
        "print(len(valid_pos_inputs))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "283707\n",
            "31582\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RI6_x69UrpPk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_pos_inputs, train_pos_masks, train_pos_labels, train_neg_inputs, train_neg_masks, train_neg_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(valid_pos_inputs, valid_pos_masks, valid_pos_labels, valid_neg_inputs, valid_neg_masks, valid_neg_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sShSbfK-rw0N",
        "colab_type": "code",
        "outputId": "8887f906-0f8f-4a56-fe64-2638862f18a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(len(train_dataloader))\n",
        "print(len(validation_dataloader))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "35464\n",
            "3948\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFHbkOpYry-m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BertPairwiseClassifier(nn.Module):\n",
        "    def __init__(self, bert):\n",
        "        \n",
        "        super().__init__()\n",
        "\n",
        "        self.config = BertConfig()\n",
        "        self.num_labels = self.config.num_labels\n",
        "        self.bert = bert\n",
        "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(self.config.hidden_size, self.config.num_labels)\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None):\n",
        "\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds)\n",
        "\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMnLBtsXr3SJ",
        "colab_type": "code",
        "outputId": "3474f063-1516-44ea-c9b0-c452a0d8ae19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "model = BertPairwiseClassifier(bert)\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertPairwiseClassifier(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-oyliUaudAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pairwise_loss(pos_scores, neg_scores):\n",
        "\n",
        "    cross_entropy_loss = -torch.log(pos_scores) - torch.log(1 - neg_scores)\n",
        "\n",
        "    margin = 0.2\n",
        "\n",
        "    hinge_loss = torch.max(torch.tensor(0, dtype=torch.float).to(device), margin - pos_scores + neg_scores)\n",
        "\n",
        "    loss = (0.5 * cross_entropy_loss + 0.5 * hinge_loss)\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezhNI5PouC_k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_pairwise(model, train_dataloader, optimizer):\n",
        "\n",
        "    # Store the average loss after each epoch so we can plot them.\n",
        "    loss_values = []\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
        "\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        pos_input = batch[0].to(device)\n",
        "        pos_mask = batch[1].to(device)\n",
        "        pos_labels = batch[2].to(device)\n",
        "\n",
        "        neg_input = batch[3].to(device)\n",
        "        neg_mask = batch[4].to(device)\n",
        "        neg_labels = batch[5].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "\n",
        "        pos_scores = torch.sigmoid(model(pos_input, token_type_ids=None, attention_mask=pos_mask, labels=pos_labels))[:,1]\n",
        "        neg_scores = torch.sigmoid(model(neg_input, token_type_ids=None, attention_mask=neg_mask, labels=neg_labels))[:,1]\n",
        "\n",
        "        loss = pairwise_loss(pos_scores, neg_scores).mean()\n",
        "        \n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "        # Accumulate the training loss over all of the batches\n",
        "        total_loss += loss.item()\n",
        "    \n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    return avg_train_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TVwYxdw5Qx9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate_pairwise(model, validation_dataloader):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_loss = 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in tqdm(validation_dataloader):\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        pos_input, pos_mask, pos_labels, neg_input, neg_mask, neg_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "            pos_scores = torch.sigmoid(model(pos_input, token_type_ids=None, attention_mask=pos_mask, labels=pos_labels))[:,1]\n",
        "            neg_scores = torch.sigmoid(model(neg_input, token_type_ids=None, attention_mask=neg_mask, labels=neg_labels))[:,1]\n",
        "\n",
        "        loss = pairwise_loss(pos_scores, neg_scores).mean()\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(validation_dataloader) \n",
        "\n",
        "    return avg_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwVMd1DQyW2G",
        "colab_type": "code",
        "outputId": "b5c99ea6-cfe8-4a84-bff8-4a2ec22c6197",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "optimizer = AdamW(model.parameters(), lr=0.001)\n",
        "\n",
        "# Lowest validation lost\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "n_epochs = 2\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    # Evaluate training loss\n",
        "    train_loss = train_pairwise(model, train_dataloader, optimizer)\n",
        "    # Evaluate validation loss\n",
        "    valid_loss = validate_pairwise(model, validation_dataloader)\n",
        "    \n",
        "    # At each epoch, if the validation loss is the best\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), path + 'model/' + str(epoch+1)+'_model-bert-pairwise.pt')\n",
        "\n",
        "    print(\"\\n\\n Epoch {}:\".format(epoch+1))\n",
        "    print(\"\\t Train Loss: {}\".format(round(train_loss, 3)))\n",
        "    print(\"\\t Validation Loss: {}\\n\".format(round(valid_loss, 3)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  2%|         | 616/35464 [08:58<8:27:20,  1.14it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5hb3gwfgrYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), path + 'model/2_model-bert-pairwise.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hIxS5Hv8reH",
        "colab_type": "code",
        "outputId": "ec3bf373-b7cc-43b5-ba33-831414b8dbab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "print('Memory Usage:')\n",
        "print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
        "print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory Usage:\n",
            "Allocated: 15.1 GB\n",
            "Cached:    15.2 GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKxNW3iprhg7",
        "colab_type": "text"
      },
      "source": [
        "## **Pointwise**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0hABsFB9lRU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validset = get_sequence_df(valid_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mc1x_WQ9lCXu",
        "colab_type": "code",
        "outputId": "b031e702-38e5-4fe9-cbf1-e83bdd914dd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "trainset = get_sequence_df(train_set)\n",
        "validset = get_sequence_df(valid_set)\n",
        "\n",
        "# Get the lists of sentences and their labels.\n",
        "train_sequences = trainset.seq_clipped.values\n",
        "train_labels = trainset.label.values\n",
        "\n",
        "valid_sequences = validset.seq_clipped.values\n",
        "valid_labels = validset.label.values\n",
        "\n",
        "print(len(train_sequences))\n",
        "print(len(valid_sequences))\n",
        "\n",
        "train_sequences = train_sequences[:3000]\n",
        "train_labels = train_labels[:3000]\n",
        "\n",
        "valid_sequences = valid_sequences[:300]\n",
        "valid_labels = valid_labels[:300]\n",
        "\n",
        "max_seq_len = 512\n",
        "\n",
        "train_input = get_input_ids(train_sequences, max_seq_len)\n",
        "valid_input = get_input_ids(valid_sequences, max_seq_len)\n",
        "\n",
        "train_att_mask = get_att_mask(train_input)\n",
        "valid_att_mask = get_att_mask(valid_input)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "298401\n",
            "33143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-uCr23oIubG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # train_labels = trainset.label.values\n",
        "# # valid_labels = validset.label.values\n",
        "\n",
        "# # save_pickle(path+'/data-bert/train_labels.pickle', train_labels)\n",
        "# # save_pickle(path+'/data-bert/valid_labels.pickle', valid_labels)\n",
        "\n",
        "# save_pickle(path+'/data-bert/train_input_512.pickle', train_input)\n",
        "# save_pickle(path+'/data-bert/valid_input_512.pickle', valid_input)\n",
        "# save_pickle(path+'/data-bert/train_mask_512.pickle', train_att_mask)\n",
        "# save_pickle(path+'/data-bert/valid_mask_512.pickle', valid_att_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xI3labKDIT3n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_input = load_pickle(path+'/data-bert/train_input.pickle')\n",
        "# valid_input = load_pickle(path+'/data-bert/valid_input.pickle')\n",
        "# train_att_mask = load_pickle(path+'/data-bert/train_mask.pickle')\n",
        "# valid_att_mask = load_pickle(path+'/data-bert/valid_mask.pickle')\n",
        "\n",
        "train_input = load_pickle(path+'/data-bert/train_input_512.pickle')\n",
        "valid_input = load_pickle(path+'/data-bert/valid_input_512.pickle')\n",
        "train_att_mask = load_pickle(path+'/data-bert/train_mask_512.pickle')\n",
        "valid_att_mask = load_pickle(path+'/data-bert/valid_mask_512.pickle')\n",
        "\n",
        "train_labels = load_pickle(path+'/data-bert/train_labels.pickle')\n",
        "valid_labels = load_pickle(path+'/data-bert/valid_labels.pickle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAk07EggdjRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_input = train_input[:1000]\n",
        "train_labels = train_labels[:1000]\n",
        "train_att_mask = train_att_mask[:1000]\n",
        "\n",
        "valid_input = valid_input[:100]\n",
        "valid_labels = valid_labels[:100]\n",
        "valid_att_mask = valid_att_mask[:100]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcigYAsxypBs",
        "colab_type": "code",
        "outputId": "7aa1aab4-3ca3-42db-83e3-0deba59086e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(len(train_input))\n",
        "print(len(valid_input))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n",
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy1wj92aI4hD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "train_inputs = torch.tensor(train_input)\n",
        "validation_inputs = torch.tensor(valid_input)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(valid_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_att_mask)\n",
        "validation_masks = torch.tensor(valid_att_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P4Lb1dzK3Ko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjVUsNKVhXDn",
        "colab_type": "code",
        "outputId": "533fb223-9570-4467-a39e-497b2dfa9c31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(len(train_dataloader))\n",
        "print(len(validation_dataloader))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "125\n",
            "13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14DFmK3GM7LU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmJJeoxC4NKu",
        "colab_type": "text"
      },
      "source": [
        "## **Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpM6uIwJ5GSv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BertClassifier(nn.Module):\n",
        "    def __init__(self, bert):\n",
        "        \n",
        "        super().__init__()\n",
        "\n",
        "        # self.config = BertConfig.from_pretrained(\"/content/drive/My Drive/FiQA/model/fin_model/config.json\")\n",
        "        self.config = BertConfig()\n",
        "        self.num_labels = self.config.num_labels\n",
        "        self.bert = bert\n",
        "        # self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.classifier = nn.Linear(self.config.hidden_size, self.config.num_labels)\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None):\n",
        "\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds)\n",
        "\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
        "\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            outputs = (loss,) + outputs\n",
        "\n",
        "        return outputs  # (loss), logits, (hidden_states), (attentions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0AdliVkwBT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# # Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# # linear classification layer on top. \n",
        "# model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "# model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jKuO5y3ihOu",
        "colab_type": "code",
        "outputId": "c275f544-d7ed-44c3-a0b5-16581206cc69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# model_path = \"/content/drive/My Drive/FiQA/model/fin_model\"\n",
        "# bert = BertModel.from_pretrained(model_path)\n",
        "\n",
        "model = BertClassifier(bert)\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertClassifier(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDc45R4mjGDz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, train_dataloader, optimizer, scheduler):\n",
        "\n",
        "    # Store the average loss after each epoch so we can plot them.\n",
        "    loss_values = []\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
        "\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        logits = outputs[1]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "        # Accumulate the training loss over all of the batches\n",
        "        total_loss += loss.item()\n",
        "    \n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    acc = eval_accuracy/nb_eval_steps\n",
        "\n",
        "    return avg_train_loss, acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNhQobcGke2I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(model, validation_dataloader):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    total_loss = 0\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in tqdm(validation_dataloader):\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask,\n",
        "                            labels=b_labels)\n",
        "        \n",
        "        loss = outputs[0]\n",
        "\n",
        "        logits = outputs[1]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    acc = eval_accuracy/nb_eval_steps\n",
        "    avg_loss = total_loss / len(validation_dataloader) \n",
        "\n",
        "    return avg_loss, acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut7wlYPSlZcM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "\n",
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ganaSXbFeMbZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 2\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3lU_QLEl4lY",
        "colab_type": "code",
        "outputId": "d6199551-db97-4368-9527-63eaade1655b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "source": [
        "# Lowest validation lost\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    # Evaluate training loss\n",
        "    train_loss, train_acc = train(model, train_dataloader, optimizer, scheduler)\n",
        "    \n",
        "    # Evaluate validation loss\n",
        "    valid_loss, valid_acc = validate(model, validation_dataloader)\n",
        "    \n",
        "    # At each epoch, if the validation loss is the best\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "    torch.save(model.state_dict(), path + str(epoch+1)+'_model-bert-full.pt')\n",
        "\n",
        "    print(\"\\n\\n Epoch {}:\".format(epoch+1))\n",
        "    print(\"\\t Train Loss: {} | Train Accuracy: {}%\".format(round(train_loss, 3), round(train_acc*100, 2)))\n",
        "    print(\"\\t Validation Loss: {} | Validation Accuracy: {}%\\n\".format(round(valid_loss, 3), round(valid_acc*100, 2)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|| 125/125 [00:58<00:00,  2.13it/s]\n",
            "100%|| 13/13 [00:01<00:00,  7.32it/s]\n",
            "  0%|          | 0/125 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " Epoch 1:\n",
            "\t Train Loss: 0.17 | Train Accuracy: 96.8%\n",
            "\t Validation Loss: 0.099 | Validation Accuracy: 98.08%\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|| 125/125 [00:58<00:00,  2.13it/s]\n",
            "100%|| 13/13 [00:01<00:00,  7.33it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " Epoch 2:\n",
            "\t Train Loss: 0.103 | Train Accuracy: 97.4%\n",
            "\t Validation Loss: 0.291 | Validation Accuracy: 87.5%\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_y_hXQeACoNT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for row in test_set:\n",
        "    row[2] = [x for x in row[2] if x is not 0]\n",
        "\n",
        "for row in test_set_full:\n",
        "    row[2] = [x for x in row[2] if x is not 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCGbsz18-Urm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_rank(model, test_set, qid_rel, max_seq_len):\n",
        "\n",
        "    qid_pred_rank = {}\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for i, seq in enumerate(tqdm(test_set)):\n",
        "        \n",
        "        qid, label, cands = seq[0], seq[1], seq[2]\n",
        "\n",
        "        q_text = add_question_token(qid_to_text[qid])\n",
        "\n",
        "        cands_id = np.array(cands)\n",
        "\n",
        "        scores = []\n",
        "\n",
        "        for docid in cands:\n",
        "\n",
        "            ans_text = add_ans_token(label_to_ans[docid])\n",
        "\n",
        "            seq_text = clip(q_text + ans_text)\n",
        "\n",
        "            encoded_seq = tokenizer.convert_tokens_to_ids(seq_text)\n",
        "\n",
        "            input_ids = pad_seq(encoded_seq, max_seq_len)\n",
        "\n",
        "            att_mask = torch.tensor([[int(token_id > 0) for token_id in input_ids]]).to(device)\n",
        "            \n",
        "            input_ids = torch.tensor([input_ids]).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "            # Forward pass, calculate logit predictions\n",
        "                outputs = model(input_ids, token_type_ids=None, attention_mask=att_mask)\n",
        "\n",
        "            logits = outputs[0]\n",
        "\n",
        "            pred = torch.sigmoid(logits)\n",
        "\n",
        "            # Move logits and labels to CPU\n",
        "            pred = pred.detach().cpu().numpy()\n",
        "\n",
        "            scores.append(pred[:,1][0])\n",
        "\n",
        "        print(scores)\n",
        "\n",
        "        # Get the indices of the sorted similarity scores\n",
        "        sorted_index = np.argsort(scores)[::-1]\n",
        "\n",
        "        # Get the docid from the sorted indices\n",
        "        ranked_ans = cands_id[sorted_index]\n",
        "\n",
        "        # Dict - key: qid, value: ranked list of docids\n",
        "        qid_pred_rank[qid] = ranked_ans\n",
        "\n",
        "    return qid_pred_rank"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOR-OM-qbb3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "toy_test_label = dict(itertools.islice(test_qid_rel.items(), 5))\n",
        "toy_test = test_set[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OchamjZk_UkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# toy_test = [[1, [14255], [84963, 354716, 14255, 522619]]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGa0KvT4cbbo",
        "colab_type": "code",
        "outputId": "b95ba61b-c5d8-4eb9-9b6f-0f5db1a37bdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "model.load_state_dict(torch.load(path+'1_model-bert-full.pt'))\n",
        "\n",
        "qid_pred_rank = get_rank(model, toy_test, toy_test_label, max_seq_len=512)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 20%|        | 1/5 [00:10<00:43, 10.87s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0.09448364, 0.091464505, 0.09531899, 0.0935531, 0.08623368, 0.0944412, 0.08836689, 0.09304453, 0.088268794, 0.10072528, 0.094937, 0.093893334, 0.094075166, 0.094597846, 0.08860202, 0.093044005, 0.09131927, 0.098692544, 0.09282827, 0.09076655, 0.09082153, 0.09292794, 0.09316235, 0.09599658, 0.0929803, 0.09787936, 0.09260589, 0.09704332, 0.09380808, 0.09536452, 0.085752115, 0.09798474, 0.09313285, 0.085965045, 0.09459156, 0.094858445, 0.09067017, 0.091676995, 0.09342291, 0.09825895, 0.09526684, 0.09755189, 0.097160704, 0.08859482, 0.09224049, 0.093643315, 0.081098124, 0.09534079, 0.101478726, 0.090585925, 0.09185903, 0.090043984, 0.092732616, 0.09962214, 0.09344325, 0.09312794, 0.084370345, 0.09385696, 0.0914924, 0.09742052, 0.100261, 0.08776788, 0.091749705, 0.09152291, 0.09483297, 0.09555992, 0.08925647, 0.096978545, 0.09085019, 0.09232709, 0.09558287, 0.09908514, 0.09034418, 0.08477161, 0.08711484, 0.0859758, 0.09212534, 0.092701495, 0.0954817, 0.09008179, 0.09259034, 0.08682558, 0.08984155, 0.09206246, 0.09919583, 0.09698293, 0.09958765, 0.091629796, 0.091504954, 0.09154601, 0.09081669, 0.09278015, 0.092606224, 0.09153978, 0.0906945, 0.09052708, 0.09010548, 0.08692898, 0.10134949, 0.0804232, 0.09249573, 0.08765486, 0.08921205, 0.08532001, 0.09739603, 0.090635695, 0.0882589, 0.090138525, 0.093027204, 0.093863, 0.097217165, 0.08716129, 0.09505341, 0.10692283, 0.09149072, 0.09089001, 0.088932775, 0.09328326, 0.10023351, 0.09423856, 0.091993004, 0.086556956, 0.09771965, 0.08790272, 0.09457075, 0.09743415, 0.092458874, 0.08754085, 0.08737373, 0.09178882, 0.088244505, 0.091858566, 0.08997234, 0.08891444, 0.08879556, 0.099567994, 0.09790014, 0.0941672, 0.09249756, 0.09098155, 0.08935992, 0.0908759, 0.09476713, 0.09632211, 0.08950923, 0.08990374, 0.08706057, 0.09660433, 0.09228504, 0.09018564, 0.090348825, 0.08948896, 0.08234625, 0.093393646, 0.093335055, 0.0857979, 0.0890494, 0.08966289, 0.09389365, 0.096443824, 0.09701013, 0.08912387, 0.098005734, 0.08747241, 0.092616886, 0.0955866, 0.08692302, 0.10120288, 0.095016435, 0.08169866, 0.09700077, 0.0854358, 0.092280306, 0.093240134, 0.092626564, 0.08865661, 0.09291661, 0.09500411, 0.09297263, 0.10017499, 0.091556475, 0.09511919, 0.091154955, 0.09235004, 0.09011095, 0.09417725, 0.08606917, 0.08999764, 0.091874614, 0.093941584, 0.09882095, 0.088729165, 0.09235996, 0.08857098, 0.09280813, 0.0950994, 0.091729365, 0.09416551, 0.09756113, 0.091590926, 0.09585, 0.09187973, 0.090947814, 0.09509718, 0.08650897, 0.099195614, 0.08749575, 0.09807779, 0.10020551, 0.098224014, 0.09495807, 0.10127194, 0.086638816, 0.10142428, 0.09442665, 0.09155507, 0.08277087, 0.09052199, 0.09741578, 0.09649138, 0.10124684, 0.09051984, 0.09904624, 0.090128705, 0.087396815, 0.09614667, 0.09099036, 0.08355893, 0.08502866, 0.09148147, 0.08738109, 0.09092739, 0.09233534, 0.08759657, 0.095387205, 0.09808156, 0.09152751, 0.09550019, 0.09577984, 0.09107936, 0.09665998, 0.09655499, 0.08839368, 0.09043365, 0.09224759, 0.10004255, 0.08895314, 0.088729955, 0.094426006, 0.09070689, 0.0877184, 0.09366836, 0.088093296, 0.09358287, 0.093765736, 0.08654329, 0.093032196, 0.097734906, 0.09371963, 0.089494, 0.08687846, 0.08854013, 0.09585294, 0.09293642, 0.087572455, 0.09862347, 0.09150582, 0.1000699, 0.08568264, 0.087490164, 0.08480147, 0.10161979, 0.09318345, 0.091234416, 0.09199595, 0.088073954, 0.09728398, 0.09526212, 0.08686912, 0.0881235, 0.09318798, 0.093434684, 0.09781075, 0.09070599, 0.097018525, 0.09513648, 0.09774193, 0.0979138, 0.09388657, 0.08884993, 0.09243014, 0.093219034, 0.09756445, 0.08980573, 0.10151934, 0.090015665, 0.09399593, 0.09483684, 0.092289306, 0.097651705, 0.09056705, 0.09023861, 0.09600835, 0.09492391, 0.09270146, 0.08583974, 0.08748092, 0.09565666, 0.09026703, 0.09621418, 0.09500141, 0.091109246, 0.0874288, 0.08939627, 0.088980615, 0.09173777, 0.10000514, 0.09469559, 0.086422764, 0.08480965, 0.096473284, 0.08918832, 0.0945732, 0.09534175, 0.08667203, 0.09360117, 0.10412077, 0.0924415, 0.09107442, 0.08905321, 0.08519748, 0.09698979, 0.09031468, 0.08366546, 0.087279454, 0.090484254, 0.09233396, 0.08693329, 0.0904934, 0.08442025, 0.0913233, 0.099742986, 0.09111853, 0.09893509, 0.09213412, 0.094749905, 0.09129172, 0.08682637, 0.089064, 0.091106676, 0.083652586, 0.092782155, 0.090431914, 0.09824125, 0.08904629, 0.09174801, 0.090850964, 0.09399458, 0.09578018, 0.08865302, 0.09506862, 0.090987004, 0.09696303, 0.088465884, 0.09037649, 0.09306248, 0.09256172, 0.09369537, 0.091416106, 0.088204615, 0.09647168, 0.08830796, 0.089391835, 0.09368255, 0.09585623, 0.09150881, 0.10230528, 0.09097851, 0.0891409, 0.0926035, 0.09401761, 0.09563785, 0.09482557, 0.094336994, 0.0938113, 0.09949155, 0.08507004, 0.091928475, 0.08943167, 0.09075382, 0.09524561, 0.09320468, 0.101111405, 0.09101003, 0.0941072, 0.0904916, 0.09221681, 0.09761022, 0.09306678, 0.08795568, 0.093857005, 0.08799681, 0.09074833, 0.09568286, 0.09112402, 0.094894566, 0.091569066, 0.0891903, 0.0827332, 0.10401778, 0.09178921, 0.10181599, 0.09499975, 0.09702742, 0.09529818, 0.0935286, 0.0815595, 0.09533443, 0.09238759, 0.09759723, 0.08332943, 0.08515993, 0.08686787, 0.08593689, 0.08774104, 0.08978554, 0.09406614, 0.102655, 0.095079, 0.09666275, 0.098952256, 0.09674533, 0.11005061, 0.09835131, 0.09135291, 0.09364864, 0.08969394, 0.09501326, 0.099279195, 0.0966246, 0.08319177, 0.10528372, 0.087899856, 0.0898566, 0.10025955, 0.08943831, 0.097275354, 0.108962454, 0.09182704, 0.090224184, 0.08411993, 0.09031233, 0.09159082, 0.088147305, 0.09692089, 0.09337517, 0.08590693, 0.08985503, 0.08877112, 0.08803172, 0.09794438, 0.087228574, 0.0908318, 0.09472267, 0.09434608, 0.091669604, 0.095938355, 0.09639243, 0.09135127, 0.08962516, 0.09058127, 0.09576756, 0.09559943, 0.089744434, 0.088422954, 0.09466395, 0.08399874, 0.089550674, 0.08610034, 0.10530198, 0.09039969, 0.09517402, 0.08364849, 0.09041964, 0.097884074, 0.09491617, 0.09086678, 0.09914826, 0.0900722, 0.088149555, 0.09622533, 0.09036029, 0.09169519, 0.099570744, 0.09238658, 0.09398019, 0.09662984, 0.09065179, 0.1021521, 0.09388312]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 40%|      | 2/5 [00:21<00:32, 10.87s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0.089038074, 0.087245695, 0.09686424, 0.090947896, 0.092642576, 0.089993894, 0.093893245, 0.090898484, 0.09217901, 0.08610029, 0.09310555, 0.093487635, 0.091983184, 0.08404333, 0.09254097, 0.09248821, 0.09485202, 0.0915587, 0.0966721, 0.0961607, 0.087108955, 0.09980025, 0.0997337, 0.089376, 0.089792706, 0.091721185, 0.09007152, 0.10155326, 0.09090053, 0.10064456, 0.10233374, 0.096182086, 0.095291115, 0.08861255, 0.0946176, 0.09427746, 0.08927234, 0.093150005, 0.08318458, 0.09034387, 0.09614644, 0.09459395, 0.094437726, 0.093337454, 0.093813345, 0.0964419, 0.102040835, 0.095333956, 0.09024939, 0.09653397, 0.089940116, 0.09411199, 0.096570194, 0.09758782, 0.09678069, 0.091985755, 0.08375582, 0.097223885, 0.091557294, 0.08654513, 0.09883658, 0.09191178, 0.089872785, 0.08798662, 0.098162286, 0.09278356, 0.08697244, 0.0869087, 0.097522974, 0.087528646, 0.08916158, 0.10115958, 0.09528215, 0.091390185, 0.088485666, 0.082588404, 0.09563435, 0.087906964, 0.097468555, 0.09543912, 0.08826826, 0.08712643, 0.090420455, 0.09698911, 0.09324461, 0.09498811, 0.09540703, 0.08640729, 0.08989546, 0.08685467, 0.07937138, 0.086709484, 0.09205921, 0.089842804, 0.09920017, 0.090318926, 0.09009991, 0.09300884, 0.10095553, 0.09096653, 0.09192846, 0.094786495, 0.08800732, 0.08611858, 0.09008217, 0.09335606, 0.09740767, 0.083586924, 0.08854083, 0.09501016, 0.0938277, 0.097591884, 0.09000518, 0.091012634, 0.09298901, 0.08653324, 0.09058106, 0.093162574, 0.09279342, 0.09105674, 0.09002611, 0.085401356, 0.08803805, 0.09089689, 0.087386526, 0.09033482, 0.0893742, 0.09282953, 0.093328655, 0.0925937, 0.09412571, 0.09681344, 0.09163236, 0.09550387, 0.091077894, 0.10307671, 0.088960394, 0.093416005, 0.08770159, 0.08675247, 0.083694056, 0.089752555, 0.09147727, 0.09429396, 0.09185894, 0.08989417, 0.097055726, 0.0927331, 0.08993302, 0.09587284, 0.09932907, 0.09315424, 0.09366661, 0.09125404, 0.08886391, 0.09688901, 0.08583702, 0.09270668, 0.09048785, 0.094594344, 0.082592286, 0.09805298, 0.10115407, 0.10182321, 0.10065084, 0.093731746, 0.091840744, 0.09135839, 0.08292377, 0.09418881, 0.09154851, 0.097246975, 0.09825324, 0.098181576, 0.10625598, 0.09525254, 0.08746871, 0.09428895, 0.09742303, 0.09040512, 0.09897973, 0.092023484, 0.08609379, 0.09410331, 0.09529052, 0.089196846, 0.091081396, 0.1065803, 0.096192494, 0.09082934, 0.09514046, 0.09637499, 0.0875344, 0.09445146, 0.08892746, 0.091564335, 0.09666769, 0.0897481, 0.090712674, 0.09539552, 0.09051097, 0.0966663, 0.09221691, 0.089837015, 0.09162424, 0.09667793, 0.08826605, 0.097687095, 0.098057956, 0.08772465, 0.08652178, 0.09425649, 0.090429515, 0.09374606, 0.08982345, 0.08923957, 0.09997835, 0.10032982, 0.08977022, 0.09323019, 0.08691513, 0.09483389, 0.085942544, 0.09293146, 0.09945774, 0.09405437, 0.091268934, 0.08978478, 0.091712244, 0.101235844, 0.08790687, 0.10653729, 0.09251407, 0.10130239, 0.09047294, 0.08673677, 0.09545405, 0.0928014, 0.08939464, 0.09293958, 0.095199525, 0.08863608, 0.092487134, 0.091403134, 0.100119345, 0.09590904, 0.09534802, 0.09652313, 0.09465533, 0.09478625, 0.09213621, 0.09763662, 0.09274219, 0.09231132, 0.08546659, 0.088488795, 0.09734134, 0.08344701, 0.088781744, 0.08585524, 0.09592431, 0.092409916, 0.09081461, 0.07856606, 0.087615624, 0.09453428, 0.088006265, 0.09325342, 0.08694976, 0.090985864, 0.091015294, 0.09800755, 0.094064616, 0.08980849, 0.08879415, 0.08375369, 0.08783661, 0.08935464, 0.103990786, 0.08781969, 0.08869256, 0.08791203, 0.08734596, 0.087125614, 0.08251132, 0.09058548, 0.092577115, 0.09239446, 0.0832189, 0.08429305, 0.090644374, 0.09626411, 0.09337743, 0.08661658, 0.08196003, 0.109017804, 0.09580466, 0.09672468, 0.09383127, 0.09164972, 0.09281181, 0.08943359, 0.09195772, 0.094271705, 0.09188546, 0.09295447, 0.0910449, 0.08857497, 0.093740106, 0.09061702, 0.088262595, 0.092622876, 0.091634184, 0.08774303, 0.09521021, 0.09122313, 0.085440695, 0.0937174, 0.09649596, 0.09862309, 0.09004356, 0.090078965, 0.09653136, 0.0862291, 0.09168006, 0.09448197, 0.08474822, 0.09303881, 0.08873899, 0.09769703, 0.09446882, 0.10153592, 0.10501905, 0.090404294, 0.09447999, 0.09594986, 0.091990344, 0.09068219, 0.09015016, 0.098274626, 0.10949921, 0.091428936, 0.08165823, 0.094530985, 0.09181177, 0.089742385, 0.097858176, 0.09672915, 0.09781601, 0.087515354, 0.09286899, 0.0949706, 0.094762914, 0.096391276, 0.09204464, 0.093707666, 0.0889365, 0.091985434, 0.09475064, 0.091678746, 0.0981866, 0.09681684, 0.09394577, 0.10145729, 0.093508296, 0.088415675, 0.09333249, 0.09040345, 0.088370465, 0.08968305, 0.09887321, 0.100911975, 0.0919347, 0.095859036, 0.09703737, 0.08797839, 0.09313986, 0.09007576, 0.08862215, 0.11216432, 0.09446437, 0.09737604, 0.093475975, 0.094152585, 0.08906108, 0.090524495, 0.090894476, 0.085280836, 0.09194229, 0.09008225, 0.0951129, 0.089545645, 0.090920456, 0.092803076, 0.08903151, 0.09348565, 0.095734335, 0.093392245, 0.08698332, 0.090580136, 0.095532715, 0.09901701, 0.09430627, 0.089510046, 0.095014445, 0.08853166, 0.086324416, 0.081752375, 0.08719413, 0.09734404, 0.08940403, 0.09173692, 0.088455476, 0.08539687, 0.08836689, 0.08247887, 0.09366371, 0.08833601, 0.094236165, 0.09901304, 0.09649424, 0.092916764, 0.09724398, 0.08946066, 0.09649426, 0.10074176, 0.089431785, 0.08437515, 0.09009678, 0.0919668, 0.0972291, 0.09250507, 0.08796391, 0.09736117, 0.09289412, 0.08817321, 0.101847984, 0.09007419, 0.09454309, 0.10314579, 0.09342433, 0.09272697, 0.08843771, 0.088323, 0.09397159, 0.08965119, 0.088845536, 0.08642161, 0.09337901, 0.0930451, 0.09356706, 0.09048706, 0.09636212, 0.09545728, 0.0922667, 0.0959642, 0.09529979, 0.08823194, 0.09210881, 0.089771904, 0.092023045, 0.08160853, 0.09102926, 0.08974197, 0.09067736, 0.09077507, 0.09399235, 0.09856469, 0.10556589, 0.09533832, 0.088726066, 0.09381769, 0.08867773, 0.10512583, 0.08918747, 0.101316094, 0.09571662, 0.08851036, 0.09136016, 0.09742968, 0.0929236, 0.08946491, 0.09079597, 0.09532161, 0.10004717, 0.08882247, 0.08900678, 0.0974102, 0.08797456, 0.090212606, 0.09190788, 0.09280476, 0.090125136, 0.10151029, 0.09354625, 0.08853675, 0.09762587, 0.0923038, 0.09000592, 0.09900824]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 60%|    | 3/5 [00:32<00:21, 10.86s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0.08202664, 0.08568311, 0.09017658, 0.09526386, 0.096064754, 0.09406523, 0.08122158, 0.08740375, 0.09736456, 0.09248821, 0.085536025, 0.092340924, 0.090646796, 0.09027831, 0.08648409, 0.08956306, 0.092648506, 0.08822748, 0.094515994, 0.09280088, 0.093344755, 0.08665109, 0.092667185, 0.0940455, 0.08530649, 0.0976262, 0.089112744, 0.09485264, 0.09445631, 0.09315919, 0.09150314, 0.090109564, 0.08813301, 0.09674544, 0.09239764, 0.08385653, 0.08625316, 0.08744082, 0.08598483, 0.090867676, 0.08790739, 0.09706946, 0.08963014, 0.09425612, 0.09472933, 0.095610715, 0.09271945, 0.09560556, 0.10084954, 0.08711935, 0.09293156, 0.09433718, 0.08842018, 0.089276105, 0.092068575, 0.0874609, 0.0907906, 0.09506944, 0.096134074, 0.09049435, 0.09294961, 0.09199798, 0.084740415, 0.098958835, 0.08937444, 0.08042217, 0.092728786, 0.093305245, 0.09156391, 0.07172713, 0.07710424, 0.09171755, 0.08841608, 0.09191569, 0.08755051, 0.08577477, 0.09135618, 0.09266591, 0.09588268, 0.0903715, 0.09597814, 0.101081364, 0.09725746, 0.09391363, 0.091303624, 0.09036556, 0.08170155, 0.08801442, 0.08646968, 0.09517476, 0.09147693, 0.09583989, 0.09532125, 0.08765953, 0.081983976, 0.09036813, 0.089614674, 0.08756552, 0.09635894, 0.09519472, 0.09228567, 0.09586552, 0.08077811, 0.085015826, 0.09093523, 0.09049164, 0.092509806, 0.09303212, 0.089247905, 0.090556785, 0.09123372, 0.08792977, 0.08999028, 0.09581805, 0.08987109, 0.091686845, 0.08444218, 0.09266152, 0.086239666, 0.08837453, 0.08713018, 0.09622483, 0.09067156, 0.09114874, 0.1062642, 0.089099504, 0.09254214, 0.09173471, 0.09001499, 0.09053959, 0.08874262, 0.09545038, 0.08596084, 0.08829113, 0.09006283, 0.092651114, 0.08072777, 0.09665595, 0.0871858, 0.08867187, 0.09027113, 0.091403194, 0.08245131, 0.085899234, 0.089632615, 0.09327571, 0.08962115, 0.08959766, 0.087775074, 0.09055352, 0.09690987, 0.08886977, 0.103077404, 0.0921321, 0.09068327, 0.09345364, 0.08957408, 0.09461122, 0.09720993, 0.091551214, 0.080751255, 0.0907347, 0.08528322, 0.0914184, 0.09114105, 0.0901824, 0.091632396, 0.0936287, 0.09026565, 0.08393483, 0.09753078, 0.08879189, 0.09370949, 0.08889107, 0.09174007, 0.08798374, 0.087730125, 0.085824005, 0.08965033, 0.0929253, 0.08983056, 0.094341144, 0.08974787, 0.08976767, 0.08875978, 0.08721435, 0.08511574, 0.08423882, 0.09216651, 0.09300957, 0.09749126, 0.09233624, 0.089628175, 0.090919495, 0.09174452, 0.09031405, 0.085114606, 0.08765789, 0.08935746, 0.09048138, 0.092458665, 0.088587806, 0.08600215, 0.086948544, 0.09033955, 0.08590876, 0.090027966, 0.09789738, 0.092076726, 0.09996092, 0.09389622, 0.08682359, 0.09375583, 0.09086638, 0.08738917, 0.08528803, 0.09389425, 0.09536941, 0.094341666, 0.09195521, 0.09689246, 0.09113148, 0.10266675, 0.09116266, 0.08895045, 0.08968534, 0.08575109, 0.0971614, 0.09041518, 0.095838495, 0.0949577, 0.102331996, 0.09092871, 0.08936646, 0.085283846, 0.09275342, 0.09150735, 0.09216761, 0.08346627, 0.09067539, 0.09188948, 0.092313685, 0.089825876, 0.0854764, 0.091964684, 0.08692902, 0.0891154, 0.09147899, 0.09092002, 0.09128333, 0.0945292, 0.10507036, 0.08782942, 0.086628675, 0.09471605, 0.09199193, 0.090576984, 0.08839729, 0.09232588, 0.086390086, 0.08884476, 0.088491775, 0.086316615, 0.087576434, 0.09252664, 0.09458509, 0.088453665, 0.09325035, 0.083150916, 0.091053426, 0.09309121, 0.080636606, 0.09644282, 0.09021293, 0.09727644, 0.08713665, 0.09573393, 0.08911315, 0.09529311, 0.0893091, 0.084620886, 0.09456453, 0.08416294, 0.09128725, 0.08697637, 0.084616736, 0.09441088, 0.09073218, 0.10554945, 0.08816612, 0.089897424, 0.08874386, 0.095475435, 0.09176493, 0.08785054, 0.091595165, 0.07643686, 0.093206175, 0.09109692, 0.09349921, 0.09375392, 0.09126917, 0.09205089, 0.09030694, 0.08457072, 0.08906927, 0.07999238, 0.089550175, 0.08605548, 0.087488875, 0.092211075, 0.090422556, 0.082193226, 0.08800223, 0.09282465, 0.09644914, 0.087876156, 0.08899205, 0.10284019, 0.08364114, 0.0903372, 0.087916695, 0.09027686, 0.089903474, 0.08500166, 0.09738115, 0.08918974, 0.096293025, 0.09204523, 0.08866104, 0.08371159, 0.0924677, 0.089704126, 0.08849035, 0.08797206, 0.09162972, 0.08980569, 0.09212714, 0.08314611, 0.09377702, 0.08629997, 0.08336534, 0.095600985, 0.094320215, 0.09736098, 0.09742651, 0.08452208, 0.09051495, 0.09449296, 0.086812794, 0.09216636, 0.08640747, 0.08780654, 0.090532474, 0.09813035, 0.08237231, 0.093398444, 0.09834158, 0.09015939, 0.08675169, 0.08385212, 0.10005786, 0.08888418, 0.08518687, 0.10051612, 0.08935592, 0.09368958, 0.083246626, 0.09329405, 0.090156436, 0.08519211, 0.09206058, 0.09602865, 0.08226667, 0.096251786, 0.093180485, 0.08934188, 0.08551575, 0.09037478, 0.10311804, 0.09134262, 0.10002863, 0.09073655, 0.08430666, 0.08133047, 0.09739256, 0.089676976, 0.090203464, 0.09044469, 0.09497315, 0.08967514, 0.09823945, 0.088912465, 0.092620805, 0.0865001, 0.09167889, 0.09420418, 0.09796138, 0.09580465, 0.09311728, 0.09806378, 0.09068851, 0.09300729, 0.09052849, 0.091681585, 0.07940008, 0.097404584, 0.08779949, 0.08448852, 0.08632248, 0.10020605, 0.09354696, 0.09081542, 0.093103796, 0.08865091, 0.10415411, 0.09448107, 0.09122354, 0.09027823, 0.09368693, 0.09280448, 0.080850594, 0.098461494, 0.09167787, 0.09213635, 0.09176775, 0.092398465, 0.093763426, 0.089973845, 0.092264004, 0.096899696, 0.09584377, 0.107732326, 0.1004912, 0.087570034, 0.08524142, 0.089664504, 0.097135544, 0.090335645, 0.088892795, 0.08725798, 0.094965294, 0.094299465, 0.092875175, 0.09178769, 0.103205964, 0.09041096, 0.09111405, 0.08990002, 0.087853566, 0.0928152, 0.0940307, 0.09781693, 0.10223976, 0.08085003, 0.09775265, 0.08981865, 0.09738072, 0.09533006, 0.09454255, 0.096135154, 0.090484776, 0.09032032, 0.08537654, 0.08821612, 0.08767511, 0.08321905, 0.0957983, 0.08048823, 0.08791174, 0.099028245, 0.09004118, 0.1044279, 0.09708563, 0.09091925, 0.08979755, 0.08655055, 0.09278149, 0.08843698, 0.07612715, 0.0926658, 0.09384907, 0.08120803, 0.08676836, 0.10071437, 0.08743497, 0.098563485, 0.08874199, 0.09677448, 0.088291556, 0.09515723, 0.08511085, 0.086246334, 0.086026885, 0.082748815, 0.08246443, 0.09030724, 0.09191776, 0.07650028, 0.08642262]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r 80%|  | 4/5 [00:43<00:10, 10.86s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0.08825335, 0.101886, 0.092105426, 0.08922268, 0.09045948, 0.09309741, 0.08942796, 0.100345545, 0.09602147, 0.098036185, 0.08480027, 0.09468512, 0.09117491, 0.09034348, 0.09715811, 0.08716859, 0.092287794, 0.09813702, 0.0976157, 0.09295471, 0.089064606, 0.08913281, 0.087866575, 0.09903978, 0.084161416, 0.0975043, 0.09257793, 0.091103576, 0.08756962, 0.08769317, 0.093012266, 0.093756296, 0.09539414, 0.09252364, 0.09320732, 0.08504195, 0.08974177, 0.098553464, 0.10199982, 0.09665653, 0.08894987, 0.09428364, 0.08838431, 0.08442191, 0.090665646, 0.08710642, 0.095091455, 0.09346481, 0.08461243, 0.09154452, 0.100219674, 0.090355866, 0.09303701, 0.09172611, 0.091345705, 0.098321326, 0.09169278, 0.096176766, 0.085551746, 0.09531286, 0.087229915, 0.097901665, 0.090055674, 0.0923282, 0.084133625, 0.104487225, 0.09031674, 0.090192296, 0.09840611, 0.09317138, 0.08864275, 0.08518538, 0.095237195, 0.08590501, 0.09496602, 0.09142436, 0.09197719, 0.08614815, 0.0937525, 0.09265928, 0.080283955, 0.09196168, 0.08870308, 0.087782696, 0.08977233, 0.08927263, 0.092720665, 0.089836605, 0.086236805, 0.08738958, 0.08516683, 0.09346365, 0.089337565, 0.0934313, 0.09522604, 0.092863746, 0.09266845, 0.09678508, 0.09675377, 0.09346386, 0.09243198, 0.09060522, 0.091541074, 0.08641803, 0.09251681, 0.09104658, 0.095874295, 0.09697158, 0.093770124, 0.09859185, 0.08534005, 0.09181653, 0.09340418, 0.0936087, 0.0922956, 0.09820874, 0.095074, 0.09127625, 0.08497928, 0.091186985, 0.09579326, 0.093567975, 0.09419062, 0.09152616, 0.0912688, 0.095455885, 0.08614832, 0.09561695, 0.089297675, 0.09435592, 0.091137305, 0.092451595, 0.09485232, 0.09780509, 0.09496056, 0.095424876, 0.09116073, 0.09018674, 0.08692882, 0.0901699, 0.093007416, 0.08575785, 0.093107834, 0.08977395, 0.093464136, 0.09532205, 0.09370642, 0.08452052, 0.10629189, 0.098201506, 0.09417259, 0.091999315, 0.09528024, 0.09274953, 0.096069105, 0.08674272, 0.09256826, 0.09309095, 0.09930968, 0.09114443, 0.08784906, 0.09501385, 0.09425476, 0.09165316, 0.09565483, 0.09767097, 0.09246965, 0.08982415, 0.088463224, 0.09297357, 0.09435829, 0.099645436, 0.095933326, 0.10084135, 0.10250144, 0.09151718, 0.088932574, 0.09416037, 0.09085521, 0.08669144, 0.08552403, 0.095197074, 0.093754366, 0.091252975, 0.09633471, 0.086481944, 0.10083887, 0.0924337, 0.08853717, 0.09942083, 0.09432563, 0.09058284, 0.090779044, 0.09131751, 0.09011842, 0.09398231, 0.08885484, 0.089929655, 0.095821664, 0.09251115, 0.093049735, 0.09042184, 0.09504443, 0.08839481, 0.092493966, 0.09769375, 0.09226343, 0.0880356, 0.097301945, 0.09030221, 0.09722679, 0.094279826, 0.08789148, 0.088037305, 0.09521738, 0.09222884, 0.091922864, 0.09645003, 0.09074688, 0.09788501, 0.09521851, 0.090287864, 0.10001242, 0.096266314, 0.094498575, 0.094168566, 0.10163661, 0.09823764, 0.09029978, 0.08604862, 0.09555619, 0.09674283, 0.08695277, 0.08747192, 0.092277706, 0.092002586, 0.091757044, 0.092004076, 0.09174259, 0.089750044, 0.09030461, 0.08543852, 0.092614084, 0.09506629, 0.088693105, 0.09762169, 0.09371323, 0.09832432, 0.092460975, 0.091330685, 0.09147053, 0.08740902, 0.09205669, 0.09513448, 0.090387516, 0.09844181, 0.09137252, 0.08577663, 0.104413435, 0.09190038, 0.09063689, 0.08862973, 0.08899567, 0.08868509, 0.09649378, 0.08543513, 0.1007194, 0.10078939, 0.089250326, 0.0880641, 0.08927492, 0.09844787, 0.09195933, 0.09282174, 0.079934865, 0.08929258, 0.094827846, 0.092676006, 0.09088129, 0.10096983, 0.08887275, 0.09533791, 0.094451636, 0.08929795, 0.08749397, 0.09460874, 0.09548932, 0.094196066, 0.08855834, 0.09362253, 0.09819272, 0.089479625, 0.08679054, 0.086349376, 0.08906645, 0.094651975, 0.08248308, 0.08636046, 0.0956377, 0.08790489, 0.09257322, 0.08747964, 0.09526035, 0.09675175, 0.08948358, 0.09796298, 0.098146096, 0.089774005, 0.08824048, 0.08626712, 0.08979106, 0.09574821, 0.09026093, 0.08913345, 0.09436483, 0.093178555, 0.09228556, 0.090546556, 0.095321864, 0.08466821, 0.09101967, 0.09311398, 0.08562345, 0.09922608, 0.09810423, 0.08878528, 0.088551395, 0.08544783, 0.08245819, 0.08944952, 0.09028828, 0.0919161, 0.103406295, 0.093980275, 0.0892471, 0.09550242, 0.08560794, 0.09564934, 0.09041048, 0.09062437, 0.095467076, 0.09304634, 0.090535484, 0.09329978, 0.090417944, 0.09345598, 0.092731215, 0.0964727, 0.09883553, 0.08401547, 0.08967529, 0.10128547, 0.08610113, 0.092642136, 0.09095621, 0.09015824, 0.09217597, 0.08953519, 0.093244836, 0.08725678, 0.08917249, 0.10077077, 0.087727584, 0.094492696, 0.09200735, 0.09627927, 0.10104337, 0.08782734, 0.08949308, 0.09413392, 0.086062565, 0.09213615, 0.09107976, 0.087979384, 0.09811774, 0.09320134, 0.09570343, 0.09219656, 0.091777064, 0.093080856, 0.089887574, 0.09282931, 0.092833124, 0.094085924, 0.09288556, 0.091794044, 0.095235035, 0.093830034, 0.092050955, 0.09357314, 0.0930154, 0.09226746, 0.09278494, 0.094342925, 0.09123291, 0.093476795, 0.094652385, 0.0903776, 0.09253082, 0.09056246, 0.093183406, 0.08741808, 0.09471298, 0.092858426, 0.08821217, 0.09907544, 0.09671004, 0.09218207, 0.09578115, 0.095842116, 0.09394291, 0.092240386, 0.09052475, 0.098844305, 0.09029604, 0.09222908, 0.09464999, 0.09061561, 0.09301403, 0.09231036, 0.08368717, 0.096163146, 0.093384966, 0.09186678, 0.08815505, 0.10090626, 0.09640596, 0.10077248, 0.0872553, 0.09346131, 0.093479335, 0.09205068, 0.09210502, 0.10261049, 0.08756462, 0.10224747, 0.09748215, 0.09339227, 0.0894559, 0.09196821, 0.10197261, 0.1018763, 0.09116602, 0.09736679, 0.08995569, 0.09116796, 0.09361645, 0.094833255, 0.08855443, 0.09127159, 0.09275898, 0.092623316, 0.091333315, 0.09085837, 0.085700475, 0.0898897, 0.10316446, 0.08656616, 0.09674303, 0.09682021, 0.09252704, 0.08921643, 0.0882738, 0.091693126, 0.09659897, 0.09125932, 0.08817467, 0.08338855, 0.09320696, 0.08269413, 0.09929028, 0.09044212, 0.09580188, 0.098117165, 0.089678705, 0.08637018, 0.099037044, 0.0925501, 0.093786456, 0.087072454, 0.09472053, 0.09209849, 0.09278952, 0.102042, 0.08901956, 0.086127415, 0.08787071, 0.088332534, 0.09428199, 0.08903353, 0.09841629, 0.091441154, 0.091900654, 0.09017954, 0.08596776, 0.088413574, 0.09062325, 0.091838785, 0.08915222, 0.094142474]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|| 5/5 [00:54<00:00, 10.86s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0.08976066, 0.09207014, 0.09648833, 0.09693424, 0.09231177, 0.09587623, 0.09568729, 0.09172134, 0.0854301, 0.085862175, 0.09642867, 0.088303484, 0.088249706, 0.09505611, 0.09269714, 0.09839677, 0.0833062, 0.08633762, 0.09099159, 0.09132982, 0.0929807, 0.08458787, 0.099441245, 0.09223015, 0.090655856, 0.10531307, 0.09141298, 0.112626895, 0.09821563, 0.083793834, 0.09458689, 0.09187455, 0.09185038, 0.08913043, 0.079459906, 0.08939326, 0.08727894, 0.087101, 0.091798656, 0.087874606, 0.10029497, 0.096677616, 0.08835464, 0.08555743, 0.0893991, 0.09152963, 0.09093151, 0.08821685, 0.084498346, 0.0912962, 0.089443825, 0.09526552, 0.09559642, 0.09019402, 0.092288226, 0.09382763, 0.09221077, 0.0926451, 0.0975671, 0.09019159, 0.086794145, 0.09204574, 0.095602676, 0.09369971, 0.08994653, 0.09076586, 0.09545356, 0.09875387, 0.09183202, 0.09166267, 0.096439086, 0.08584152, 0.090720855, 0.09433995, 0.089876145, 0.090831034, 0.09440039, 0.091119945, 0.092928275, 0.09070891, 0.08970008, 0.09155034, 0.08221073, 0.08834301, 0.09381773, 0.094034754, 0.086750716, 0.092508115, 0.09717658, 0.097488776, 0.08708193, 0.0905325, 0.097009294, 0.09275441, 0.08864473, 0.081801236, 0.09469681, 0.09328388, 0.095578745, 0.10183071, 0.093127534, 0.10083346, 0.09693174, 0.09037646, 0.09708847, 0.091268376, 0.090719946, 0.097344756, 0.08336703, 0.09036363, 0.100324035, 0.09111477, 0.10047403, 0.090419844, 0.089546256, 0.094258316, 0.08953818, 0.09607031, 0.10677843, 0.08057886, 0.10480744, 0.09160085, 0.09743268, 0.09753357, 0.08871309, 0.09329743, 0.09581381, 0.09422891, 0.088663526, 0.08766464, 0.096848026, 0.08942794, 0.08739521, 0.08208416, 0.09217303, 0.08880881, 0.094519876, 0.0869307, 0.10185933, 0.091077045, 0.091778405, 0.07917155, 0.09211888, 0.09172861, 0.09315069, 0.09122425, 0.09446308, 0.09209197, 0.09241715, 0.09426546, 0.095173694, 0.093308024, 0.09575395, 0.10339931, 0.08266914, 0.09895556, 0.08914985, 0.09404913, 0.09011137, 0.09614098, 0.09943994, 0.08624421, 0.088145, 0.09022523, 0.094381414, 0.086646326, 0.08939719, 0.09159609, 0.085891694, 0.0911986, 0.09638209, 0.07850137, 0.09694396, 0.095110156, 0.09175813, 0.087427944, 0.09294082, 0.09049488, 0.09328549, 0.09416678, 0.08228269, 0.08586158, 0.09104457, 0.09278868, 0.08958005, 0.08905793, 0.08717393, 0.09480697, 0.09470799, 0.095654786, 0.0974678, 0.094785556, 0.10374554, 0.105643906, 0.08884476, 0.08852055, 0.09526246, 0.09193957, 0.093943395, 0.09610582, 0.08232474, 0.09134105, 0.08508992, 0.094057865, 0.099712506, 0.095627174, 0.09271784, 0.0954323, 0.08911671, 0.092691585, 0.08325365, 0.08256295, 0.099097736, 0.09179087, 0.09135435, 0.0889411, 0.08959162, 0.088499986, 0.095215045, 0.09863775, 0.094316855, 0.08227722, 0.09430286, 0.09612138, 0.08492323, 0.103559256, 0.0949771, 0.09315349, 0.09440876, 0.08833668, 0.09304728, 0.08991455, 0.08755969, 0.09540419, 0.08284951, 0.08802103, 0.092412174, 0.09405974, 0.08794792, 0.10263223, 0.088089354, 0.09190931, 0.087272435, 0.09595413, 0.09011978, 0.08636, 0.08788886, 0.09313533, 0.085519984, 0.08849377, 0.098535255, 0.086500436, 0.093106195, 0.09256951, 0.08804269, 0.08818586, 0.09531202, 0.088962, 0.08977345, 0.093028836, 0.088912636, 0.09187882, 0.09344155, 0.0875526, 0.086670786, 0.09767969, 0.09095845, 0.09208312, 0.10635113, 0.08760221, 0.09678011, 0.084169984, 0.11020381, 0.09452783, 0.08358318, 0.095693134, 0.09303326, 0.08913502, 0.08946375, 0.09195706, 0.09626234, 0.08953993, 0.09049785, 0.09068061, 0.08408948, 0.0899844, 0.0945226, 0.08321928, 0.088272095, 0.09281213, 0.09938459, 0.087376066, 0.10028656, 0.094200335, 0.08896842, 0.09562051, 0.08825621, 0.09188832, 0.08437349, 0.09476862, 0.092104055, 0.09778158, 0.09260985, 0.08904677, 0.093583845, 0.09272612, 0.09743712, 0.10725964, 0.08407461, 0.08609258, 0.08488491, 0.09663259, 0.087598555, 0.094985545, 0.08673193, 0.09460316, 0.09391329, 0.10055001, 0.08640535, 0.097928695, 0.09293964, 0.093131036, 0.091611326, 0.094010696, 0.09913069, 0.08547644, 0.09096964, 0.09033773, 0.09990414, 0.09409107, 0.09108504, 0.09600813, 0.090924926, 0.09513488, 0.08114088, 0.087057866, 0.08891904, 0.08236609, 0.07842878, 0.09079121, 0.092660815, 0.092366695, 0.08726063, 0.092607066, 0.08859276, 0.0975376, 0.09282675, 0.085872784, 0.0921426, 0.087953694, 0.091984674, 0.094789974, 0.0922706, 0.10196984, 0.09569115, 0.09400896, 0.090821736, 0.089849114, 0.08394847, 0.09225345, 0.09099257, 0.09441082, 0.09463406, 0.08950351, 0.08729139, 0.09520521, 0.0794269, 0.09660314, 0.09321697, 0.09209397, 0.09837972, 0.0906209, 0.08975611, 0.08854602, 0.085446455, 0.08700308, 0.0891879, 0.088914156, 0.08697293, 0.09355897, 0.09489966, 0.09432661, 0.09160424, 0.096434906, 0.09400247, 0.08713518, 0.09007296, 0.092759825, 0.09060525, 0.08785508, 0.09000946, 0.097270995, 0.09247975, 0.0919199, 0.08941662, 0.099196315, 0.0878742, 0.093746245, 0.078561224, 0.09265626, 0.07271889, 0.09239811, 0.098526485, 0.083444364, 0.10094436, 0.09752973, 0.09456608, 0.088160254, 0.09030334, 0.089209594, 0.09454673, 0.097924836, 0.09298713, 0.086543195, 0.09606136, 0.10028286, 0.08631705, 0.084547065, 0.093127996, 0.09340717, 0.09794139, 0.099310026, 0.08779465, 0.08192735, 0.089002304, 0.09338688, 0.09187018, 0.08818109, 0.08467032, 0.08651764, 0.08606626, 0.09450555, 0.09645746, 0.09632254, 0.09042598, 0.09335685, 0.088044256, 0.08650221, 0.07984261, 0.091781795, 0.08172015, 0.08909134, 0.09620004, 0.09109276, 0.093919635, 0.08757674, 0.10564593, 0.09839918, 0.096162625, 0.09696237, 0.100662194, 0.092498936, 0.09275446, 0.088763215, 0.09462961, 0.088826485, 0.0959617, 0.08589741, 0.09568022, 0.09265097, 0.08740337, 0.083367884, 0.093312114, 0.08791307, 0.08865014, 0.08993311, 0.090248264, 0.082556956, 0.1009676, 0.09352148, 0.086252496, 0.0933998, 0.08555956, 0.08602578, 0.093272425, 0.093612544, 0.10349632, 0.090904735, 0.096514486, 0.08443513, 0.08456016, 0.089379065, 0.08533659, 0.08859052, 0.08637933, 0.089282, 0.09518367, 0.094503, 0.09002104, 0.09068513, 0.09017905, 0.0954681, 0.11209089, 0.09744444, 0.09294285, 0.087154336, 0.08193822, 0.08734434, 0.09102628, 0.09260374]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsjYl50Ij6ok",
        "colab_type": "code",
        "outputId": "4a494c2e-ea26-4473-a974-47cba97c91ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(test_set[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, [14255], [84963, 354716, 522619, 418999, 322064, 141738, 303078, 355897, 71987, 219313, 310612, 257168, 410431, 541809, 466718, 89190, 66356, 329209, 283505, 596289, 362060, 113632, 113776, 362069, 129355, 381151, 81343, 46791, 360925, 294738, 157233, 263521, 509862, 90290, 598547, 397608, 442968, 81599, 68969, 434619, 71569, 476980, 365558, 251392, 118615, 292748, 133701, 246461, 327002, 510692, 368263, 304452, 224167, 234436, 98636, 107794, 388713, 524879, 399762, 174321, 282958, 245447, 345070, 361978, 183612, 566417, 539511, 19640, 420295, 525149, 302049, 12729, 223170, 41793, 356884, 288995, 472824, 597053, 40628, 546277, 262960, 275312, 112793, 248448, 363495, 223697, 406789, 349674, 550345, 11132, 265527, 189642, 18934, 527776, 314161, 576985, 146388, 156554, 424720, 109546, 531442, 144190, 399882, 469043, 89008, 494000, 296345, 193367, 299971, 234743, 189765, 354314, 128861, 431010, 158409, 523564, 177074, 257738, 213041, 283113, 187227, 533808, 444899, 507107, 38249, 202179, 216783, 589416, 136804, 274721, 399848, 445298, 187586, 74774, 192726, 216745, 283079, 185999, 585341, 298970, 91325, 15643, 490489, 94600, 5762, 483664, 141458, 527037, 55666, 513051, 96158, 18539, 30343, 375423, 103405, 441038, 584232, 590775, 160340, 103668, 237207, 547087, 487551, 299211, 87113, 458930, 24421, 18850, 192843, 75005, 174025, 179144, 440061, 326717, 304559, 97719, 542213, 484231, 583757, 144783, 390368, 254158, 192516, 512151, 392484, 351672, 413681, 395011, 577284, 576218, 88967, 521044, 18889, 461526, 532888, 147080, 411825, 391841, 47260, 328853, 408112, 414834, 328073, 547941, 523415, 77171, 479542, 109250, 560776, 424001, 211485, 507371, 21846, 252758, 542139, 559671, 78486, 331384, 100280, 588253, 388667, 289582, 252273, 182989, 86134, 401731, 446553, 373481, 402437, 515690, 581265, 153377, 113876, 204554, 483942, 18570, 385506, 253210, 357717, 18001, 483385, 389164, 322906, 536849, 438515, 344093, 364938, 588211, 197205, 79397, 555732, 195207, 205791, 81886, 55500, 289620, 5587, 272248, 216008, 194899, 510181, 73427, 221281, 166183, 513362, 544381, 19455, 434846, 120500, 285255, 334902, 391619, 458079, 291079, 162500, 280021, 69317, 77325, 197501, 476173, 178942, 365456, 47957, 202457, 173212, 522671, 474795, 169613, 442142, 197352, 462831, 509346, 552632, 338096, 97489, 377621, 155648, 427469, 245011, 121063, 447231, 278460, 349672, 55200, 510716, 307779, 131453, 28764, 274870, 72053, 264659, 172745, 420846, 401832, 79411, 374443, 352640, 396056, 215761, 110117, 561750, 70738, 359814, 390559, 316932, 426639, 220621, 155490, 152407, 535673, 349424, 528564, 208989, 454537, 468741, 297841, 286245, 255101, 154931, 402404, 333954, 571062, 413976, 422199, 297938, 487728, 423083, 251649, 219274, 328745, 427849, 248761, 519473, 42665, 249450, 338700, 485102, 596598, 37636, 306600, 309909, 453455, 426343, 248629, 546509, 158738, 121551, 202645, 191848, 74688, 311285, 177946, 540395, 132780, 441568, 541682, 65771, 421301, 98072, 203905, 395139, 402327, 397920, 76618, 481692, 152027, 156063, 583956, 528838, 276934, 187384, 390435, 212661, 208219, 167494, 209997, 290265, 231279, 481114, 444273, 520922, 444589, 216077, 510144, 377547, 424008, 196374, 576362, 492456, 388042, 593760, 366074, 130631, 291717, 319234, 105543, 399115, 334603, 562802, 182168, 462184, 243822, 54742, 151645, 254151, 124009, 540325, 296717, 588327, 363178, 531578, 483489, 376129, 153541, 316074, 576295, 245810, 529853, 562777, 15270, 378484, 279149, 83346, 571232, 31117, 375357, 18647, 146317, 470066, 183891, 34810, 417981, 233751, 477476, 194308, 361637, 411063, 156444, 99745, 100764, 175889, 8200, 85622, 397152, 574417, 55261, 406418, 534997, 571711, 176054, 42924, 16911, 40257, 12822, 174714, 358631, 178501, 33544, 304980, 83004, 235823, 313361, 253541, 227757, 331898, 223624, 107584, 197175, 441419, 434351, 507871, 489633, 452248, 59000, 61782, 599336, 108062, 201546, 146657, 14255]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz9y45bpFUW1",
        "colab_type": "code",
        "outputId": "e1d81eef-2898-4697-d755-ec933308e676",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "qid_pred_rank"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: array([513362, 388042, 479542, 191848, 366074, 462184,  96158, 399762,\n",
              "         21846, 141738, 413681, 264659, 399115, 444273, 304452, 361637,\n",
              "        156063, 377621, 441038,  18934, 546509, 507371, 235823,  75005,\n",
              "        133701, 550345, 146388, 219313, 220621, 539511,  99745, 254158,\n",
              "        328853,  81886, 192843, 319234, 109250, 381151, 431010, 333954,\n",
              "        363495, 109546,  16911, 205791, 177074, 173212,  98072, 197175,\n",
              "        243822, 357717,  14255, 265527, 376129, 540325, 253541, 197205,\n",
              "        174025, 593760, 245011,  12729,  18539,  69317, 524879, 209997,\n",
              "        183612, 216077,  72053, 458079, 373481, 529853, 197352, 297938,\n",
              "         28764, 166183, 364938, 216783, 344093, 253210, 158738,  89190,\n",
              "        571232, 338096, 378484, 157233,  68969,  24421, 356884, 476980,\n",
              "        494000,  87113, 576295, 427849, 286245, 583757, 588211, 128861,\n",
              "         79397, 278460, 424001,  65771, 424008, 176054, 510181, 187384,\n",
              "        216008, 322906, 410431,  74688, 520922, 172745, 461526, 297841,\n",
              "        248761, 487551,  15643, 202457, 179144, 487728,  31117, 132780,\n",
              "        329209,  78486, 510692, 162500, 306600, 534997, 434619, 365456,\n",
              "        589416, 349674, 385506, 584232, 202645, 314161, 406418, 566417,\n",
              "        107584, 542139, 310612, 158409, 155490,  47957, 427469, 263521,\n",
              "        476173,   5762, 113876, 178942,  55500,  74774, 151645, 509862,\n",
              "        302049, 334902, 401832, 458930, 422199, 453455,  42665, 313361,\n",
              "        213041, 274721, 483942, 375357, 248629,  97489, 288995, 583956,\n",
              "        174714, 234436, 470066, 399848, 391841, 175889, 445298, 251392,\n",
              "        527776, 577284, 411825, 311285, 362069, 598547,  18850, 574417,\n",
              "         94600,  55200, 221281, 509346, 523564, 585341, 279149, 304980,\n",
              "        421301, 490489, 484231, 355897, 224167, 581265, 249450, 349424,\n",
              "        397608, 107794, 257738,  33544,  86134,  47260,  55261, 483385,\n",
              "        112793, 246461, 392484, 442142, 208219, 299971, 510144,  71569,\n",
              "        441568,  83004, 283113, 187227, 100280,  18001, 276934, 194308,\n",
              "        426639, 388713, 528564,  54742, 144190,  81599,  11132, 528838,\n",
              "         38249, 215761, 160340,  90290, 438515, 596289, 365558, 195207,\n",
              "        507107, 183891, 178501, 540395, 446553, 291717, 212661,  61782,\n",
              "        189642, 338700, 147080,  85622,   8200, 424720, 327002, 352640,\n",
              "        576362, 219274, 472824, 411063,  77325,  19455, 194899, 390435,\n",
              "        252758,  42924, 121063, 397920, 368263,  46791, 248448, 510716,\n",
              "        189765, 414834,   5587, 169613, 146317, 309909, 552632, 334603,\n",
              "        196374, 349672, 395139, 359814, 154931, 280021, 245810, 442968,\n",
              "         77171, 328745, 113776, 187586, 396056, 223624, 562802, 326717,\n",
              "        283079, 391619, 408112, 420295, 227757, 588253, 531578, 204554,\n",
              "        254151, 156444, 231279, 531442, 234743, 144783, 292748, 418999,\n",
              "        474795, 108062, 216745,  98636,  76618, 441419, 272248,  88967,\n",
              "        590775, 390559, 535673, 559671, 291079,  73427, 389164,  71987,\n",
              "        252273, 100764, 283505, 294738, 462831, 201546, 533808, 597053,\n",
              "        105543, 523415, 452248, 103668, 345070,  37636, 483664, 331384,\n",
              "        354314, 426343, 363178,  12822, 434351, 397152,  89008, 413976,\n",
              "        296717, 296345, 390368, 237207, 374443, 289582, 507871, 146657,\n",
              "         18647, 521044, 185999,  40257, 251649, 469043, 468741, 420846,\n",
              "        304559, 547941,  84963, 434846, 152407, 402437, 331898, 223697,\n",
              "        298970, 182989, 406789, 257168,  18570,  41793, 423083, 192726,\n",
              "        299211, 599336, 197501, 444899,  18889, 571711, 182168, 174321,\n",
              "        492456, 522619, 388667, 262960, 562777, 275312, 402327, 141458,\n",
              "         55666, 153377, 360925, 155648, 485102,  19640, 519473, 282958,\n",
              "        233751, 527037, 576218, 202179, 399882, 193367, 454537, 402404,\n",
              "         91325, 203905, 285255, 245447, 223170, 153541, 571062, 522671,\n",
              "        110117, 576985, 118615, 361978, 512151, 544381, 541809, 167494,\n",
              "        596598, 401731, 547087,  97719, 525149, 316074,  83346, 481114,\n",
              "        113632, 316932,  15270, 358631, 351672,  40628, 131453, 555732,\n",
              "        152027, 130631, 377547, 354716, 541682,  81343, 532888, 466718,\n",
              "        322064, 546277, 307779, 290265, 103405, 211485, 328073, 362060,\n",
              "        477476, 536849, 156554, 447231, 375423,  79411, 560776, 417981,\n",
              "        483489,  59000,  30343, 177946, 440061, 289620, 274870, 395011,\n",
              "        515690, 489633, 208989, 120500,  70738, 513051,  66356, 303078,\n",
              "        124009, 255101, 136804, 481692, 444589,  34810, 129355, 561750,\n",
              "        588327, 542213, 192516, 121551])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHjUlJszDAd7",
        "colab_type": "code",
        "outputId": "899266bc-d31b-4488-de18-bcf92ac45e93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "k = 500\n",
        "\n",
        "num_q = len(toy_test)\n",
        "\n",
        "MRR, average_ndcg, precision = evaluate(qid_pred_rank, toy_test_label, k)\n",
        "# MRR, average_ndcg, precision = evaluate(qid_pred_rank, test_qid_rel, k)\n",
        "\n",
        "print(\"\\n\\nAverage nDCG@{} for {} queries: {}\\n\".format(k, num_q, average_ndcg))\n",
        "\n",
        "print(\"MRR@{} for {} queries: {}\\n\".format(k, num_q, MRR))\n",
        "\n",
        "print(\"Average Precision@{}: {}\".format(1, precision))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Average nDCG@500 for 5 queries: 0.1352267643706147\n",
            "\n",
            "MRR@500 for 5 queries: 0.008511731568069007\n",
            "\n",
            "Average Precision@1: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcfbXb5eBcX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_pickle(path+'rank/2_bert_test_full.pickle', qid_pred_rank)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}