{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "colab_type": "code",
    "id": "ySXHdHsEUxNd",
    "outputId": "746aba93-a37b-4d12-f7a1-e5d5cbf851f0"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 885
    },
    "colab_type": "code",
    "id": "1-hWS0v0sx7J",
    "outputId": "043a6642-0a4d-4ce3-908a-60206e9e257f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in ./.local/lib/python3.7/site-packages (2.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.7/site-packages (from transformers) (4.40.2)\n",
      "Requirement already satisfied: sentencepiece in ./.local/lib/python3.7/site-packages (from transformers) (0.1.85)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.7/site-packages (from transformers) (1.17.4)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.7/site-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: sacremoses in ./.local/lib/python3.7/site-packages (from transformers) (0.0.38)\n",
      "Requirement already satisfied: boto3 in ./.local/lib/python3.7/site-packages (from transformers) (1.12.2)\n",
      "Requirement already satisfied: tokenizers==0.0.11 in ./.local/lib/python3.7/site-packages (from transformers) (0.0.11)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.7/site-packages (from transformers) (2019.8.19)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.7/site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/anaconda3/lib/python3.7/site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.7/site-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.7/site-packages (from requests->transformers) (1.24.2)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.7/site-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in ./.local/lib/python3.7/site-packages (from boto3->transformers) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in ./.local/lib/python3.7/site-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.2 in ./.local/lib/python3.7/site-packages (from boto3->transformers) (1.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/anaconda3/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.2->boto3->transformers) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /opt/anaconda3/lib/python3.7/site-packages (from botocore<1.16.0,>=1.15.2->boto3->transformers) (0.15.2)\n",
      "Using device: cuda\n",
      "\n",
      "Tesla P100-PCIE-16GB\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import pandas as pd\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import random\n",
    "!pip install transformers\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# Setting device on GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Set the random seed manually for reproducibility.\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "path = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MBGukeI_cCPy"
   },
   "outputs": [],
   "source": [
    "from evaluate import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HbIIxiVhU1VL"
   },
   "outputs": [],
   "source": [
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n",
    "\n",
    "def remove_empty(test_set):\n",
    "    for index, row in enumerate(test_set):\n",
    "        for doc in row[1]:\n",
    "            if doc in empty_docs:\n",
    "                del test_set[index]\n",
    "    return test_set\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def save_pickle(path, data):\n",
    "    with open(path, 'wb') as handle:\n",
    "        pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def pad_seq(seq, max_seq_len):\n",
    "    # Pad each seq to be the same length to process in batch.\n",
    "    # pad_token = 0\n",
    "    if len(seq) >= max_seq_len:\n",
    "        seq = seq[:max_seq_len]\n",
    "    else:\n",
    "        seq += [0]*(max_seq_len - len(seq))\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7iCcUYUvb6-r"
   },
   "outputs": [],
   "source": [
    "# dict mapping of token to idx\n",
    "vocab = load_pickle(path + 'vocab_full.pickle')\n",
    "# dict mapping of docid to doc text\n",
    "docid_to_text = load_pickle(path + 'label_ans.pickle')\n",
    "\n",
    "# dict mapping of qid to question text\n",
    "qid_to_text = load_pickle(path + 'qid_text.pickle')\n",
    "\n",
    "train_qid_rel = load_pickle(path + \"qid_rel_train.pickle\")\n",
    "test_qid_rel = load_pickle(path + \"qid_rel_test.pickle\")\n",
    "valid_qid_rel = load_pickle(path + \"qid_rel_valid.pickle\")\n",
    "\n",
    "train_set = load_pickle(path + 'data_train_50.pickle')\n",
    "valid_set = load_pickle(path + 'data_valid_50.pickle')\n",
    "\n",
    "test_set = load_pickle(path + 'data_test_500_rel.pickle')\n",
    "test_set_full = load_pickle(path + 'data_test_500.pickle')\n",
    "\n",
    "empty_docs = load_pickle(path+'empty_docs.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "B9_sM2Lrb794",
    "outputId": "ac78c405-1b71-4788-e56c-b5f340187a29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 283707\n",
      "Number of validation samples: 31582\n",
      "Number of test samples: 330\n"
     ]
    }
   ],
   "source": [
    "train_set = [x for x in train_set if x[1] not in empty_docs]\n",
    "valid_set = [x for x in valid_set if x[1] not in empty_docs]\n",
    "\n",
    "test_set = remove_empty(test_set)\n",
    "test_set_full = remove_empty(test_set_full)\n",
    "\n",
    "print(\"Number of training samples: {}\".format(len(train_set)))\n",
    "print(\"Number of validation samples: {}\".format(len(valid_set)))\n",
    "print(\"Number of test samples: {}\".format(len(test_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85,
     "referenced_widgets": [
      "3d763ae797f94699b8ee7a2c5b9c4243",
      "959cad564e414167915a8c8b52119f65",
      "b254734589b742e0bcda7375fde020bc",
      "80471ba96275477c9f38889af2ebee6e",
      "1903567596324c4bb1d2eb457706dde9",
      "d4a5e19bfd47438582b8c4d17137d85f",
      "68328bff023c4e12800897a62f26af4c",
      "47833dff0af7450588de83821a007eb0"
     ]
    },
    "colab_type": "code",
    "id": "6npVgMDNewIW",
    "outputId": "5b024e58-4a99-47e8-ab90-eda3401b3f88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lXwjgq4zuPzg"
   },
   "outputs": [],
   "source": [
    "label_to_ans = load_pickle(\"data-bert/label_to_ans.pickle\")\n",
    "qid_to_text = load_pickle(\"data-bert/qid_to_text.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KZERfpbehdwj"
   },
   "outputs": [],
   "source": [
    "def add_question_token(q_tokens):\n",
    "    c = [\"[CLS]\"]\n",
    "    s = [\"[SEP]\"]\n",
    "    q_tokens = c + q_tokens\n",
    "    q_tokens = q_tokens + s\n",
    "\n",
    "    return q_tokens\n",
    "\n",
    "def add_ans_token(a_tokens):\n",
    "    s = [\"[SEP]\"]\n",
    "    a_tokens = a_tokens + s\n",
    "\n",
    "    return a_tokens\n",
    "\n",
    "def clip(lst):\n",
    "    max_seq_len = 512\n",
    "    if len(lst) > max_seq_len:\n",
    "        lst = lst[:max_seq_len]\n",
    "    else:\n",
    "        lst = lst\n",
    "    \n",
    "    return lst\n",
    "\n",
    "def get_input_ids(sequences, max_seq_len):\n",
    "    # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "    input_ids = []\n",
    "\n",
    "    for seq in sequences:\n",
    "        # `encode` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Map tokens to their IDs.\n",
    "        encoded_seq = tokenizer.convert_tokens_to_ids(seq)\n",
    "        \n",
    "        # Add the encoded sentence to the list.\n",
    "        input_ids.append(encoded_seq)\n",
    "\n",
    "    input_ids = pad_sequences(input_ids, maxlen=max_seq_len, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\")\n",
    "    return input_ids\n",
    "\n",
    "def get_att_mask(input_ids):\n",
    "    # Create attention masks\n",
    "    attention_masks = []\n",
    "\n",
    "    # For each sentence...\n",
    "    for sent in input_ids:\n",
    "        \n",
    "        # Create the attention mask.\n",
    "        #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
    "        #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "        \n",
    "        # Store the attention mask for this sentence.\n",
    "        attention_masks.append(att_mask)\n",
    "\n",
    "    return attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lcnub5RAVxJD"
   },
   "outputs": [],
   "source": [
    "def get_sequence_df(dataset):\n",
    "    df = pd.DataFrame(dataset)\n",
    "    df = df.rename(columns={0: 'qid', 1: 'pos', 2:'neg'})\n",
    "    df_pos = df[['qid', 'pos']]\n",
    "    df_pos = df_pos.rename(columns={'pos': 'docid'})\n",
    "    df_pos['label'] = df_pos.apply(lambda x: 1, axis=1)\n",
    "    df_pos = df_pos.drop_duplicates()\n",
    "\n",
    "    df_neg = df[['qid', 'neg']]\n",
    "    df_neg = df_neg.rename(columns={'neg': 'docid'})\n",
    "    df_neg['label'] = df_neg.apply(lambda x: 0, axis=1)\n",
    "    data_df = pd.concat([df_pos, df_neg]).sort_values(by=['qid'])\n",
    "\n",
    "    data_df['question'] = data_df['qid'].apply(lambda x: qid_to_text[x])\n",
    "    data_df['ans_cand'] = data_df['docid'].apply(lambda x: label_to_ans[x])\n",
    "    data_df['ques_token'] = data_df['question'].apply(lambda x: add_question_token(x))\n",
    "    data_df['ans_cand'] = data_df['ans_cand'].apply(lambda x: add_ans_token(x))\n",
    "\n",
    "    data_df = data_df[['qid', 'docid', 'label', 'ans_cand','ques_token']]\n",
    "    data_df['seq'] = data_df['ques_token'] + data_df['ans_cand']\n",
    "\n",
    "    data_df['seq_clipped'] = data_df['seq'].apply(clip)\n",
    "    # train['len'] = train['seq_clipped'].apply(lambda x: len(x))\n",
    "\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gd6H-9xlrS66"
   },
   "outputs": [],
   "source": [
    "def get_pairwise_sequence_df(dataset):\n",
    "    df = pd.DataFrame(dataset)\n",
    "    df = df.rename(columns={0: 'qid', 1: 'pos_id', 2:'neg_id'})\n",
    "    df['pos_label'] = df.apply(lambda x: 1, axis=1)\n",
    "    df['neg_label'] = df.apply(lambda x: 0, axis=1)\n",
    "\n",
    "    df['question'] = df['qid'].apply(lambda x: qid_to_text[x])\n",
    "    df['pos_ans'] = df['pos_id'].apply(lambda x: label_to_ans[x])\n",
    "    df['neg_ans'] = df['neg_id'].apply(lambda x: label_to_ans[x])\n",
    "\n",
    "    df['ques_token'] = df['question'].apply(lambda x: add_question_token(x))\n",
    "    df['pos_ans'] = df['pos_ans'].apply(lambda x: add_ans_token(x))\n",
    "    df['neg_ans'] = df['neg_ans'].apply(lambda x: add_ans_token(x))\n",
    "\n",
    "    df = df[['qid', 'pos_id', 'neg_id', 'pos_label', 'neg_label', 'pos_ans', 'neg_ans', 'ques_token']]\n",
    "    df['pos_seq'] = df['ques_token'] + df['pos_ans']\n",
    "    df['neg_seq'] = df['ques_token'] + df['neg_ans']\n",
    "\n",
    "    df['pos_seq_clipped'] = df['pos_seq'].apply(clip)\n",
    "    df['neg_seq_clipped'] = df['neg_seq'].apply(clip)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5YEbteNsrcM6"
   },
   "source": [
    "## **Pairwise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "CjB1w2ZjrXAb",
    "outputId": "10290662-0b2a-4bc2-fc7c-fd1cf77672fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283707\n",
      "31582\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-d3b0eda57cbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mvalid_neg_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_neg_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mtrain_pos_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_att_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pos_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mtrain_neg_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_att_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_neg_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mvalid_pos_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_att_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_pos_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-5e797bc591ba>\u001b[0m in \u001b[0;36mget_att_mask\u001b[0;34m(input_ids)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m#   - If a token ID is 0, then it's padding, set the mask to 0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m#   - If a token ID is > 0, then it's a real token, set the mask to 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0matt_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_id\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# Store the attention mask for this sentence.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-5e797bc591ba>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m#   - If a token ID is 0, then it's padding, set the mask to 0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m#   - If a token ID is > 0, then it's a real token, set the mask to 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0matt_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_id\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# Store the attention mask for this sentence.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainset = get_pairwise_sequence_df(train_set)\n",
    "validset = get_pairwise_sequence_df(valid_set)\n",
    "\n",
    "# Get the lists of sentences and their labels.\n",
    "train_pos_seq = trainset.pos_seq_clipped.values\n",
    "train_neg_seq = trainset.neg_seq_clipped.values\n",
    "train_pos_labels = trainset.pos_label.values\n",
    "train_neg_labels = trainset.neg_label.values\n",
    "\n",
    "valid_pos_seq = validset.pos_seq_clipped.values\n",
    "valid_neg_seq = validset.neg_seq_clipped.values\n",
    "valid_pos_labels = validset.pos_label.values\n",
    "valid_neg_labels = validset.neg_label.values\n",
    "\n",
    "print(len(train_pos_seq))\n",
    "print(len(valid_pos_seq))\n",
    "\n",
    "# train_pos_seq = train_pos_seq[:300]\n",
    "# train_neg_seq = train_neg_seq[:300]\n",
    "# train_pos_labels = train_pos_labels[:300]\n",
    "# train_neg_labels = train_neg_labels[:300]\n",
    "\n",
    "# valid_pos_seq = valid_pos_seq[:30]\n",
    "# valid_neg_seq = valid_neg_seq[:30]\n",
    "# valid_pos_labels = valid_pos_labels[:30]\n",
    "# valid_neg_labels = valid_neg_labels[:30]\n",
    "\n",
    "max_seq_len = 512\n",
    "\n",
    "train_pos_input = get_input_ids(train_pos_seq, max_seq_len)\n",
    "train_neg_input = get_input_ids(train_neg_seq, max_seq_len)\n",
    "valid_pos_input = get_input_ids(valid_pos_seq, max_seq_len)\n",
    "valid_neg_input = get_input_ids(valid_neg_seq, max_seq_len)\n",
    "\n",
    "train_pos_mask = get_att_mask(train_pos_input)\n",
    "train_neg_mask = get_att_mask(train_neg_input)\n",
    "valid_pos_mask = get_att_mask(valid_pos_input)\n",
    "valid_neg_mask = get_att_mask(valid_neg_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cNabMgS5-xtx"
   },
   "outputs": [],
   "source": [
    "# save_pickle(path+'/data-bert/train_pos_labels.pickle', train_pos_labels)\n",
    "# save_pickle(path+'/data-bert/train_neg_labels.pickle', train_neg_labels)\n",
    "# save_pickle(path+'/data-bert/valid_pos_labels.pickle', valid_pos_labels)\n",
    "# save_pickle(path+'/data-bert/valid_neg_labels.pickle', valid_neg_labels)\n",
    "\n",
    "save_pickle('data-bert/train_pos_input_512.pickle', train_pos_input)\n",
    "save_pickle('data-bert/train_neg_input_512.pickle', train_neg_input)\n",
    "save_pickle('data-bert/valid_pos_input_512.pickle', valid_pos_input)\n",
    "save_pickle('data-bert/valid_neg_input_512.pickle', valid_neg_input)\n",
    "\n",
    "save_pickle('data-bert/train_pos_mask_512.pickle', train_pos_mask)\n",
    "save_pickle('data-bert/train_neg_mask_512.pickle', train_neg_mask)\n",
    "save_pickle('data-bert/valid_pos_mask_512.pickle', valid_pos_mask)\n",
    "save_pickle('data-bert/valid_neg_mask_512.pickle', valid_neg_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PkpbuKiD_jlx"
   },
   "outputs": [],
   "source": [
    "train_pos_labels = load_pickle('data-bert/train_pos_labels.pickle')\n",
    "train_neg_labels = load_pickle('data-bert/train_neg_labels.pickle')\n",
    "valid_pos_labels = load_pickle('data-bert/valid_pos_labels.pickle')\n",
    "valid_neg_labels = load_pickle('data-bert/valid_neg_labels.pickle')\n",
    "\n",
    "train_pos_input = load_pickle('data-bert/train_pos_input_512.pickle')\n",
    "train_neg_input = load_pickle('data-bert/train_neg_input_512.pickle')\n",
    "valid_pos_input = load_pickle('data-bert/valid_pos_input_512.pickle')\n",
    "valid_neg_input = load_pickle('data-bert/valid_neg_input_512.pickle')\n",
    "\n",
    "train_pos_mask = load_pickle('data-bert/train_pos_mask_512.pickle')\n",
    "train_neg_mask = load_pickle('data-bert/train_neg_mask_512.pickle')\n",
    "valid_pos_mask = load_pickle('data-bert/valid_pos_mask_512.pickle')\n",
    "valid_neg_mask = load_pickle('data-bert/valid_neg_mask_512.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o1vGWJhkrkkF"
   },
   "outputs": [],
   "source": [
    "train_pos_inputs = torch.tensor(train_pos_input)\n",
    "train_neg_inputs = torch.tensor(train_neg_input)\n",
    "valid_pos_inputs = torch.tensor(valid_pos_input)\n",
    "valid_neg_inputs = torch.tensor(valid_neg_input)\n",
    "\n",
    "train_pos_labels = torch.tensor(train_pos_labels)\n",
    "train_neg_labels = torch.tensor(train_neg_labels)\n",
    "valid_pos_labels = torch.tensor(valid_pos_labels)\n",
    "valid_neg_labels = torch.tensor(valid_neg_labels)\n",
    "\n",
    "train_pos_masks = torch.tensor(train_pos_mask)\n",
    "train_neg_masks = torch.tensor(train_neg_mask)\n",
    "valid_pos_masks = torch.tensor(valid_pos_mask)\n",
    "valid_neg_masks = torch.tensor(valid_neg_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Wr11jao6rmmJ",
    "outputId": "8a8a9e7c-a64f-4b29-ca87-895866f2623a"
   },
   "outputs": [],
   "source": [
    "print(len(train_pos_inputs))\n",
    "print(len(valid_pos_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RI6_x69UrpPk"
   },
   "outputs": [],
   "source": [
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here.\n",
    "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
    "# 16 or 32.\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "# Create the DataLoader for our training set.\n",
    "train_data = TensorDataset(train_pos_inputs, train_pos_masks, train_pos_labels, train_neg_inputs, train_neg_masks, train_neg_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set.\n",
    "validation_data = TensorDataset(valid_pos_inputs, valid_pos_masks, valid_pos_labels, valid_neg_inputs, valid_neg_masks, valid_neg_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "sShSbfK-rw0N",
    "outputId": "8887f906-0f8f-4a56-fe64-2638862f18a4"
   },
   "outputs": [],
   "source": [
    "print(len(train_dataloader))\n",
    "print(len(validation_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rFHbkOpYry-m"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BertPairwiseClassifier(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.config = BertConfig()\n",
    "        self.num_labels = self.config.num_labels\n",
    "        self.bert = bert\n",
    "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(self.config.hidden_size, self.config.num_labels)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None):\n",
    "\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds)\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "SMnLBtsXr3SJ",
    "outputId": "3474f063-1516-44ea-c9b0-c452a0d8ae19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertPairwiseClassifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "model = BertPairwiseClassifier(bert)\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9-oyliUaudAR"
   },
   "outputs": [],
   "source": [
    "def pairwise_loss(pos_scores, neg_scores):\n",
    "\n",
    "    cross_entropy_loss = -torch.log(pos_scores) - torch.log(1 - neg_scores)\n",
    "\n",
    "    margin = 0.2\n",
    "\n",
    "    hinge_loss = torch.max(torch.tensor(0, dtype=torch.float).to(device), margin - pos_scores + neg_scores)\n",
    "\n",
    "    loss = (0.5 * cross_entropy_loss + 0.5 * hinge_loss)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ezhNI5PouC_k"
   },
   "outputs": [],
   "source": [
    "def train_pairwise(model, train_dataloader, optimizer):\n",
    "\n",
    "    # Store the average loss after each epoch so we can plot them.\n",
    "    loss_values = []\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        pos_input = batch[0].to(device)\n",
    "        pos_mask = batch[1].to(device)\n",
    "        pos_labels = batch[2].to(device)\n",
    "\n",
    "        neg_input = batch[3].to(device)\n",
    "        neg_mask = batch[4].to(device)\n",
    "        neg_labels = batch[5].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        pos_scores = torch.sigmoid(model(pos_input, token_type_ids=None, attention_mask=pos_mask, labels=pos_labels))[:,1]\n",
    "        neg_scores = torch.sigmoid(model(neg_input, token_type_ids=None, attention_mask=neg_mask, labels=neg_labels))[:,1]\n",
    "\n",
    "        loss = pairwise_loss(pos_scores, neg_scores).mean()\n",
    "        \n",
    "        # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "        # Accumulate the training loss over all of the batches\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    return avg_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_TVwYxdw5Qx9"
   },
   "outputs": [],
   "source": [
    "def validate_pairwise(model, validation_dataloader):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_loss = 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in tqdm(validation_dataloader):\n",
    "        \n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # Unpack the inputs from our dataloader\n",
    "        pos_input, pos_mask, pos_labels, neg_input, neg_mask, neg_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "            pos_scores = torch.sigmoid(model(pos_input, token_type_ids=None, attention_mask=pos_mask, labels=pos_labels))[:,1]\n",
    "            neg_scores = torch.sigmoid(model(neg_input, token_type_ids=None, attention_mask=neg_mask, labels=neg_labels))[:,1]\n",
    "\n",
    "        loss = pairwise_loss(pos_scores, neg_scores).mean()\n",
    "\n",
    "        # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(validation_dataloader) \n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "zwVMd1DQyW2G",
    "outputId": "b5c99ea6-cfe8-4a84-bff8-4a2ec22c6197"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|         | 616/35464 [08:58<8:27:20,  1.14it/s]"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "# Lowest validation lost\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "n_epochs = 2\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # Evaluate training loss\n",
    "    train_loss = train_pairwise(model, train_dataloader, optimizer)\n",
    "    # Evaluate validation loss\n",
    "    valid_loss = validate_pairwise(model, validation_dataloader)\n",
    "    \n",
    "    # At each epoch, if the validation loss is the best\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), path + 'model/' + str(epoch+1)+'_model-bert-pairwise.pt')\n",
    "\n",
    "    print(\"\\n\\n Epoch {}:\".format(epoch+1))\n",
    "    print(\"\\t Train Loss: {}\".format(round(train_loss, 3)))\n",
    "    print(\"\\t Validation Loss: {}\\n\".format(round(valid_loss, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_5hb3gwfgrYb"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), path + 'model/2_model-bert-pairwise.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "4hIxS5Hv8reH",
    "outputId": "ec3bf373-b7cc-43b5-ba33-831414b8dbab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Usage:\n",
      "Allocated: 15.1 GB\n",
      "Cached:    15.2 GB\n"
     ]
    }
   ],
   "source": [
    "print('Memory Usage:')\n",
    "print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OKxNW3iprhg7"
   },
   "source": [
    "## **Pointwise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "Mc1x_WQ9lCXu",
    "outputId": "320b7453-b83c-402d-ac31-ba98a12853e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298401\n",
      "33143\n"
     ]
    }
   ],
   "source": [
    "trainset = get_sequence_df(train_set)\n",
    "validset = get_sequence_df(valid_set)\n",
    "\n",
    "# Get the lists of sentences and their labels.\n",
    "train_sequences = trainset.seq_clipped.values\n",
    "train_labels = trainset.label.values\n",
    "\n",
    "valid_sequences = validset.seq_clipped.values\n",
    "valid_labels = validset.label.values\n",
    "\n",
    "print(len(train_sequences))\n",
    "print(len(valid_sequences))\n",
    "\n",
    "# train_sequences = train_sequences[:3000]\n",
    "# train_labels = train_labels[:3000]\n",
    "\n",
    "# valid_sequences = valid_sequences[:300]\n",
    "# valid_labels = valid_labels[:300]\n",
    "\n",
    "max_seq_len = 512\n",
    "\n",
    "train_input = get_input_ids(train_sequences, max_seq_len)\n",
    "valid_input = get_input_ids(valid_sequences, max_seq_len)\n",
    "\n",
    "train_att_mask = get_att_mask(train_input)\n",
    "valid_att_mask = get_att_mask(valid_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f-uCr23oIubG"
   },
   "outputs": [],
   "source": [
    "train_labels = trainset.label.values\n",
    "valid_labels = validset.label.values\n",
    "\n",
    "save_pickle('data-bert/train_labels.pickle', train_labels)\n",
    "save_pickle('data-bert/valid_labels.pickle', valid_labels)\n",
    "\n",
    "save_pickle('data-bert/train_input_512.pickle', train_input)\n",
    "save_pickle('data-bert/valid_input_512.pickle', valid_input)\n",
    "save_pickle('data-bert/train_mask_512.pickle', train_att_mask)\n",
    "save_pickle('data-bert/valid_mask_512.pickle', valid_att_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xI3labKDIT3n"
   },
   "outputs": [],
   "source": [
    "# train_input = load_pickle(path+'/data-bert/train_input.pickle')\n",
    "# valid_input = load_pickle(path+'/data-bert/valid_input.pickle')\n",
    "# train_att_mask = load_pickle(path+'/data-bert/train_mask.pickle')\n",
    "# valid_att_mask = load_pickle(path+'/data-bert/valid_mask.pickle')\n",
    "\n",
    "train_input = load_pickle('data-bert/train_input_512.pickle')\n",
    "valid_input = load_pickle('data-bert/valid_input_512.pickle')\n",
    "train_att_mask = load_pickle('data-bert/train_mask_512.pickle')\n",
    "valid_att_mask = load_pickle('data-bert/valid_mask_512.pickle')\n",
    "\n",
    "train_labels = load_pickle('data-bert/train_labels.pickle')\n",
    "valid_labels = load_pickle('data-bert/valid_labels.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Iy1wj92aI4hD"
   },
   "outputs": [],
   "source": [
    "# Convert all inputs and labels into torch tensors, the required datatype \n",
    "# for our model.\n",
    "train_inputs = torch.tensor(train_input)\n",
    "validation_inputs = torch.tensor(valid_input)\n",
    "\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(valid_labels)\n",
    "\n",
    "train_masks = torch.tensor(train_att_mask)\n",
    "validation_masks = torch.tensor(valid_att_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "IcigYAsxypBs",
    "outputId": "eb838059-ce47-457a-9225-4e361820d3b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298401\n",
      "33143\n"
     ]
    }
   ],
   "source": [
    "print(len(train_input))\n",
    "print(len(valid_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1P4Lb1dzK3Ko"
   },
   "outputs": [],
   "source": [
    "# The DataLoader needs to know our batch size for training, so we specify it \n",
    "# here.\n",
    "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
    "# 16 or 32.\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "# Create the DataLoader for our training set.\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "# Create the DataLoader for our validation set.\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "xjVUsNKVhXDn",
    "outputId": "b03c4e61-9f02-4479-89c3-5d5a7f8a33bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37301\n",
      "4143\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader))\n",
    "print(len(validation_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "14DFmK3GM7LU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nmJJeoxC4NKu"
   },
   "source": [
    "## **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QpM6uIwJ5GSv"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.config = BertConfig()\n",
    "        self.num_labels = self.config.num_labels\n",
    "        self.bert = bert\n",
    "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(self.config.hidden_size, self.config.num_labels)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None, labels=None):\n",
    "\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids, head_mask=head_mask, inputs_embeds=inputs_embeds)\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "aab583901fc64f2bb99347c4241ed340",
      "590e0e137a5f40408336300cc1c2b530",
      "df3ec5dcd9f34d6dabd4f4af50fd1c2c",
      "90171f1069794bb89292cf6b5c58d93d",
      "9aba313875d046eba46b0f78345d13ea",
      "894bb594ce08413eb0f865ec98e2035e",
      "de8f576fd9e94a59893a98763440e3bf",
      "cddf5a7330e84d4dbea9c4e84e03da2a",
      "f5d09a4ab8b84bffb37ac8d07317c877",
      "d1c34f57b8ae42b496b2d20338415f42",
      "359a0a5f99e44691901bbd10ed97d52b",
      "d04fa886080e4a0c98dfe654a6c1a1a4",
      "e98c5260bc4644a8bec4b7d1f44f0eff",
      "0d57a913dfdf4c81b5f512c1225987cb",
      "fd34cfe552ab44ff8ea619a8474b1fa2",
      "14c13046daba400aab55cc5cf1be7bd9"
     ]
    },
    "colab_type": "code",
    "id": "naoa-MQF_lwE",
    "outputId": "8dece157-bef1-4132-98a0-53cd9980b83d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertClassifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "model = BertClassifier(bert)\n",
    "\n",
    "# Tell pytorch to run this model on the GPU.\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GDc45R4mjGDz"
   },
   "outputs": [],
   "source": [
    "def train(model, train_dataloader, optimizer):\n",
    "\n",
    "    # Store the average loss after each epoch so we can plot them.\n",
    "    loss_values = []\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_loss = 0\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "\n",
    "        # `batch` contains three pytorch tensors:\n",
    "        #   [0]: input ids \n",
    "        #   [1]: attention masks\n",
    "        #   [2]: labels \n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        # This will return the loss (rather than the model output) because we\n",
    "        # have provided the `labels`.\n",
    "        outputs = model(b_input_ids, \n",
    "                    token_type_ids=None, \n",
    "                    attention_mask=b_input_mask, \n",
    "                    labels=b_labels)\n",
    "        # The call to `model` always returns a tuple, so we need to pull the \n",
    "        # loss value out of the tuple.\n",
    "        loss = outputs[0]\n",
    "\n",
    "        logits = outputs[1]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        # Accumulate the total accuracy.\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "        # Accumulate the training loss over all of the batches\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # Update parameters and take a step using the computed gradient.\n",
    "        optimizer.step()\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    avg_train_loss = total_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    loss_values.append(avg_train_loss)\n",
    "\n",
    "    acc = eval_accuracy/nb_eval_steps\n",
    "\n",
    "    return avg_train_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DNhQobcGke2I"
   },
   "outputs": [],
   "source": [
    "def validate(model, validation_dataloader):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_loss = 0\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in tqdm(validation_dataloader):\n",
    "        \n",
    "        # Add batch to GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        \n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(b_input_ids, \n",
    "                            token_type_ids=None, \n",
    "                            attention_mask=b_input_mask,\n",
    "                            labels=b_labels)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "\n",
    "        logits = outputs[1]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        \n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        \n",
    "        # Accumulate the total accuracy.\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        # Track the number of batches\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    acc = eval_accuracy/nb_eval_steps\n",
    "    avg_loss = total_loss / len(validation_dataloader) \n",
    "\n",
    "    return avg_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ut7wlYPSlZcM"
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "-3lU_QLEl4lY",
    "outputId": "01fba835-6779-4253-d580-d6df2cb0ff8c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|        | 4397/37301 [37:26<4:39:40,  1.96it/s]"
     ]
    }
   ],
   "source": [
    "# Lowest validation lost\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "n_epochs = 2\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # Evaluate training loss\n",
    "    train_loss, train_acc = train(model, train_dataloader, optimizer)\n",
    "    \n",
    "    # Evaluate validation loss\n",
    "    valid_loss, valid_acc = validate(model, validation_dataloader)\n",
    "    \n",
    "    # At each epoch, if the validation loss is the best\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'model/' + str(epoch+1)+'_model-bert-512.pt')\n",
    "\n",
    "    print(\"\\n\\n Epoch {}:\".format(epoch+1))\n",
    "    print(\"\\t Train Loss: {} | Train Accuracy: {}%\".format(round(train_loss, 3), round(train_acc*100, 2)))\n",
    "    print(\"\\t Validation Loss: {} | Validation Accuracy: {}%\\n\".format(round(valid_loss, 3), round(valid_acc*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uCVZwbcZqxl1"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model/2_model-bert-512.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "cE1ghzszv_0D",
    "outputId": "695b03e3-9e4f-4154-acc4-93192cbb7eb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla P100-PCIE-16GB\n",
      "Memory Usage:\n",
      "Allocated: 7.7 GB\n",
      "Cached:    8.0 GB\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.get_device_name(0))\n",
    "print('Memory Usage:')\n",
    "print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_y_hXQeACoNT"
   },
   "outputs": [],
   "source": [
    "for row in test_set:\n",
    "    row[2] = [x for x in row[2] if x is not 0]\n",
    "\n",
    "for row in test_set_full:\n",
    "    row[2] = [x for x in row[2] if x is not 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "TT0cIadqEdNp",
    "outputId": "cdfa4a2d-881e-4238-928c-95be93420267"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(14255, ['[CLS]', 'what', 'is', 'the', 'easiest', 'way', 'to', 'back', '-', 'test', 'index', 'funds', 'and', 'et', '##fs', '?', '[SEP]', 'yes', 'you', 'can', 'claim', 'your', 'business', 'de', '##duction', '##s', 'if', 'you', 'are', 'not', 'making', 'any', 'income', 'yet', '.', 'but', 'first', 'you', 'should', 'decide', 'what', 'structure', 'you', 'want', 'to', 'have', 'for', 'your', 'business', '.', 'either', 'a', 'company', 'structure', 'or', 'a', 'sole', 'trader', 'or', 'partnership', '.', 'company', 'structure', 'if', 'you', 'choose', 'a', 'company', 'structure', '(', 'which', 'is', 'more', 'expensive', 'to', 'set', 'up', ')', 'you', 'would', 'claim', 'your', 'de', '##duction', '##s', 'but', 'no', 'income', '.', 'so', 'you', 'would', 'be', 'making', 'a', 'loss', ',', 'and', 'continue', 'making', 'losses', 'until', 'your', 'income', 'from', 'the', 'business', 'exceed', 'your', 'expenses', '.', 'so', 'these', 'losses', 'will', 'remain', 'inside', 'the', 'company', 'and', 'can', 'be', 'carried', 'forward', 'to', 'future', 'income', 'years', 'when', 'you', 'are', 'making', 'profits', 'to', 'offset', 'these', 'profits', '.', 'refer', 'to', 'at', '##o', '-', 'company', 'tax', 'losses', 'for', 'more', 'information', '.', 'sole', 'trader', 'of', 'partnership', 'structure', 'if', 'you', 'choose', 'to', 'be', 'a', 'sole', 'trader', 'or', 'a', 'partnership', 'and', 'your', 'business', 'makes', 'a', 'loss', 'you', 'must', 'check', 'the', 'non', '-', 'commercial', 'loss', 'rules', 'to', 'see', 'if', 'you', 'can', 'offset', 'the', 'loss', 'against', 'your', 'income', 'from', 'other', 'sources', ',', 'such', 'as', 'wages', '.', 'in', 'order', 'to', 'offset', 'your', 'business', 'losses', 'against', 'your', 'other', 'income', 'your', 'business', 'must', 'pass', 'one', 'of', 'these', 'tests', ':', 'if', 'you', 'don', \"'\", 't', 'pass', 'any', 'of', 'these', 'tests', ',', 'which', 'being', 'a', 'start', '-', 'up', 'you', 'most', 'likely', 'won', \"'\", 't', ',', 'you', 'must', 'carry', 'forward', 'your', 'business', 'losses', 'until', 'an', 'income', 'year', 'in', 'which', 'you', 'do', 'pass', 'one', 'of', 'the', 'tests', ',', 'then', 'you', 'can', 'offset', 'it', 'against', 'your', 'other', 'income', '.', 'this', 'is', 'what', 'differentiate', '##s', 'a', 'legitimate', 'business', 'from', 'someone', 'having', 'a', 'hobby', ',', 'because', 'unless', 'you', 'start', 'making', 'at', 'least', '$', '20', ',', '000', 'in', 'sales', 'income', '(', 'the', 'easiest', 'test', 'to', 'pass', ')', 'you', 'cannot', 'use', 'your', 'business', 'losses', 'against', 'your', 'other', 'income', '.', 'refer', 'to', 'at', '##o', '-', 'non', '-', 'commercial', 'losses', 'for', 'more', 'information', '.', '[SEP]']), (153377, ['[CLS]', 'where', 'do', 'expense', 'ratios', 'show', 'up', 'on', 'my', 'statement', '?', '[SEP]', 'hobby', 'expenses', 'are', 'not', 'tax', 'de', '##du', '##ct', '##ible', '.', 'business', 'expenses', 'are', ',', 'but', 'only', 'if', 'it', \"'\", 's', 'a', 'bon', '##a', 'fide', 'business', '.', 'first', 'they', 'look', 'at', 'profit', '##ability', ':', 'if', 'you', 'reported', 'a', 'net', 'profit', '(', 'i', '.', 'e', '.', 'paid', 'taxes', ')', 'in', 'your', 'first', '3', 'years', ',', 'they', 'will', 'believe', 'you', 'ran', '##t', 'on', 'youtube', 'for', 'a', 'living', '.', 'remember', ',', 'by', 'the', 'time', 'they', 'get', 'around', 'to', 'audit', '##ing', 'you', ',', 'you', \"'\", 'll', 'likely', 'be', 'well', 'into', ',', 'or', 'through', ',', 'your', 'third', 'year', '.', 'there', 'is', 'an', 'exception', 'for', 'farms', '.', 'other', 'than', 'that', ',', 'if', 'you', 'lose', 'money', 'year', 'after', 'year', ',', 'you', 'better', 'be', 'able', 'to', 'show', 'that', 'you', 'look', ',', 'walk', 'and', 'qu', '##ack', 'like', 'a', 'business', ';', 'and', 'one', 'with', 'a', 'reasonable', 'business', 'reason', 'for', 'delayed', 'profit', '##ability', '.', 'for', 'instance', 'netflix', \"'\", 's', 'old', 'business', 'model', 'of', 'mail', '##ing', 'dvds', 'had', 'very', 'high', 'fixed', 'infrastructure', 'expense', 'that', 'took', 'years', 'to', 'turn', 'profitable', ',', 'but', 'was', 'a', 'very', 'sensible', 'model', '.', 'they', \"'\", 're', 'fine', 'with', 'that', '.', 'pets', '.', 'com', 'swan', '##di', '##ved', 'into', 'oblivion', 'but', 'they', 'earnest', '##ly', 'tried', '.', 'they', \"'\", 're', 'fine', 'with', 'that', 'too', '.', 'you', 'can', \"'\", 't', 'mix', 'all', 'your', 'activities', '.', 'if', 'you', \"'\", 're', 'an', 'electric', '##ian', 'specializing', 'in', 'io', '##t', 'and', 'smart', 'homes', ',', 'can', 'you', 'de', '##du', '##ct', 'a', 'trip', 'to', 'the', 'ce', '##s', 'trade', 'show', ',', 'you', 'bet', '.', 'black', '##hat', 'conference', ',', 'ar', '##gua', '##ble', '.', 'se', '##s', '?', 'no', 'way', '.', 'now', 'if', 'you', 'had', 'a', 'second', 'business', 'of', 'a', 'product', '-', 'rec', '##o', 'site', 'which', 'profit', '##ed', 'by', 'ads', 'and', 'affiliate', 'links', ',', 'then', 'se', '##s', 'would', 'be', 'fine', 'to', 'de', '##du', '##ct', 'from', 'that', 'business', '.', 'but', 'if', 'this', 'second', 'business', 'loses', 'money', 'every', 'year', ',', 'it', \"'\", 's', 'a', 'hobby', 'and', 'not', 'de', '##du', '##ct', '##ible', 'at', 'all', '.', 'that', 'person', 'would', 'want', 'separate', 'accounting', 'books', 'for', 'the', 'electric', '##ian', 'and', 'web', '##master', 'businesses', '.', 'that', \"'\", 's', 'a', 'basic', '\"', 'duck', 'test', '\"', 'of', 'a', 'business', 'vs', '.', 'a', 'hobby', '.', 'you', 'need', 'to', 'be', 'able', 'to', 'show', 'how', 'each', 'business', 'gets', 'income', 'and', 'pays', 'expense', 'separate', 'from', 'every', 'other', 'business', 'and', 'your', 'personal', 'life', '.', 'it', \"'\", 's', 'a', 'best', '-', 'practice', 'to', 'give', 'each', 'business', 'a', 'separate', 'checking', 'account', 'and', 'check', '##book', '.', 'you', 'don', \"'\", 't', 'need', 'to', 'risk', 'tax', 'penalties', 'on', 'a', 'business', '-', 'la', '##rva', 'that', 'may', 'never', 'pup', '##ate', '.', 'you', 'can', 'amend', 'your', 'taxes', 'up', 'to', '3', 'years', 'after', 'the', 'proper', 'filing', 'date', '.', 'i', 'save', 'my', 'expense', 'rec', '##ie', '##pts', 'for', 'each', 'tax', 'year', ',', 'and', 'if', 'a', 'business', 'becomes', 'just', '##if', '##iable', ',', 'i', 'go', 'back', 'and', 'amend', 'past', 'years', \"'\", 'tax', 'forms', ',', 'taking', 'those', 'de', '##duction', '##s', '.', 'irs', 'gives', 'me', 'a', 'ref', '##und', 'check', ',', 'with', 'interest', '!', '[SEP]']), (581265, ['[CLS]', 'understanding', 'sec', 'filing', '##s', '[SEP]', 'in', 'the', 'us', 'tax', 'system', ',', 'you', 'cannot', '\"', 'write', '-', 'off', '\"', 'capital', 'assets', '.', 'you', 'have', 'to', 'de', '##pre', '##cia', '##te', 'them', ',', 'with', 'very', 'specific', 'exceptions', '.', 'so', 'while', 'you', 'may', 'be', 'purchasing', '$', '450', '##0', 'of', 'equipment', ',', 'your', 'de', '##duction', 'may', 'be', 'significantly', 'less', '.', 'for', 'example', ',', 'computers', 'are', 'de', '##pre', '##cia', '##ted', 'over', 'the', 'period', 'of', '5', 'years', ',', 'so', 'if', 'you', 'bought', 'a', '$', '1000', 'computer', '-', 'you', 'write', 'off', '$', '200', '/', 'year', 'until', 'it', 'is', 'completely', 'de', '##pre', '##cia', '##ted', ',', 'not', '$', '1000', 'at', 'once', '.', 'there', 'are', 'exceptions', 'however', ',', 'for', 'example', '-', 'ir', '##c', 'sec', '.', '179', 'is', 'one', 'of', 'them', '.', 'but', 'you', 'should', 'talk', 'to', 'a', 'tax', 'adviser', '(', 'ea', '/', 'cp', '##a', 'licensed', 'in', 'your', 'state', ')', 'about', 'whether', 'it', 'is', 'applicable', 'to', 'the', 'specific', 'expense', 'you', 'want', 'to', '\"', 'write', 'off', '\"', 'and', 'to', 'what', 'extent', '.', 'also', ',', 'keep', 'in', 'mind', 'that', 'state', 'laws', 'may', 'not', 'conform', 'to', 'the', 'federal', 'ir', '##c', '.', 'while', 'you', 'may', 'be', 'able', 'to', 'use', 'sec', '.', '179', 'or', 'other', 'exceptions', 'and', 'de', '##du', '##ct', 'your', 'expenses', 'on', 'your', 'federal', 'return', ',', 'you', 'may', 'end', 'up', 'with', 'a', 'whole', 'different', 'set', 'of', 'de', '##duction', '##s', 'on', 'your', 'state', 'return', '.', 'and', 'last', 'but', 'not', 'least', ':', 'equipment', 'that', 'you', 'de', '##pre', '##cia', '##ted', 'or', 'otherwise', '\"', 'wrote', 'off', '\"', 'that', 'is', 'later', 'sold', '-', 'is', 'income', 'to', 'you', ',', 'since', 'de', '##pre', '##ciation', '/', 'de', '##duction', 'reduces', 'basis', '.', 'ah', ',', 'and', 'keep', 'in', 'mind', '-', 'the', 'irs', 'frowns', 'upon', 'schedule', 'c', 'business', 'that', 'consistently', 'show', 'losses', '.', 'if', 'you', 'have', 'losses', 'for', 'more', 'than', '3', 'in', 'the', 'last', '5', 'years', '-', 'your', 'business', 'may', 'be', 'classified', 'as', '\"', 'hobby', '\"', ',', 'and', 'de', '##duction', '##s', 'may', 'be', 'di', '##sal', '##lowe', '##d', '.', 'but', 'the', 'bottom', 'line', 'is', 'that', 'yes', ',', 'it', 'is', 'possible', 'to', 'end', 'up', 'with', '0', 'tax', 'liability', 'with', 'business', 'income', 'offset', 'by', 'business', 'de', '##duction', '##s', '.', 'however', ',', 'not', 'for', 'prolonged', 'periods', 'of', 'time', '(', 'not', 'for', 'years', 'consistently', ',', 'but', 'first', 'year', 'may', 'fly', ')', '.', 'again', '-', 'you', 'should', 'talk', 'to', 'a', 'licensed', 'tax', 'adviser', '(', 'ea', '/', 'cp', '##a', 'licensed', 'in', 'your', 'state', ')', '.', 'it', 'is', 'well', 'worth', 'the', 'money', '.', 'do', 'not', 'rely', 'on', 'answers', 'on', 'free', 'internet', 'forums', 'as', 'a', 'tax', 'advice', '-', 'it', 'is', 'not', '.', '[SEP]']), (515690, ['[CLS]', 'is', 'there', 'an', 'advantage', 'to', 'a', 'traditional', 'but', 'non', '-', 'de', '##du', '##ctable', 'ira', 'over', 'a', 'taxa', '##ble', 'account', '?', '[', 'duplicate', ']', '[SEP]', 'i', 'don', \"'\", 't', 'quite', 'understand', 'your', 'thought', 'process', 'here', '.', 'first', ',', 'in', 'a', 'tax', '-', 'advantage', '##d', 'retirement', 'account', 'you', 'are', 'not', 'allowed', 'to', 'engage', 'in', 'a', 'transaction', 'with', 'yourself', '.', 'if', 'you', 'just', 'want', 'to', 'run', 'a', 'business', 'and', 'be', 'able', 'to', 'write', 'off', 'expenses', ',', 'how', 'is', 'using', 'the', 'self', '-', 'directed', 'ira', 'relevant', '?', 'you', 'can', 'either', 'buy', 'the', 'condo', 'using', 'your', 'tax', '-', 'advantage', '##d', 'account', 'and', 'rent', 'it', 'out', 'to', 'regular', 'tenants', '.', 'or', 'you', 'buy', 'the', 'condo', 'yourself', 'using', 'your', 'own', 'money', 'and', 'then', 'operate', 'your', 'business', 'so', 'you', 'can', 'de', '##du', '##ct', 'business', 'expenses', 'from', 'doing', 'so', '.', '401', '##k', \"'\", 's', 'allow', 'you', 'to', 'take', 'a', 'loan', 'out', 'of', 'it', ',', 'so', 'you', 'can', 'look', 'into', 'that', 'as', 'well', '.', '[SEP]']), (402437, ['[CLS]', 'understanding', 'sec', 'filing', '##s', '[SEP]', '&', 'gt', ';', 'the', 'base', 'value', 'from', 'infrastructure', 'is', 'derived', 'on', 'a', 'per', '-', 'capita', 'basis', '.', 'it', 'is', 'a', '\"', 'fixed', 'cost', '\"', 'as', 'opposed', 'to', 'a', 'variable', 'one', '.', 'in', 'other', 'words', ',', 'roads', 'are', 'just', 'as', 'useful', 'to', 'me', 'as', 'they', 'are', 'to', 'you', 'regardless', 'of', 'my', 'net', 'worth', '.', 'a1', ':', 'misleading', ':', 'infrastructure', 'is', 'useful', 'to', 'those', 'who', 'use', 'it', 'more', 'independently', 'of', 'classify', '##ing', 'it', 'as', '[', 'fixed', 'vs', 'variable', ']', '(', 'http', ':', '/', '/', 'en', '.', 'wikipedia', '.', 'org', '/', 'wi', '##ki', '/', 'fixed', '_', 'cost', ')', '.', 'take', 'the', 'faa', 'for', 'example', '.', 'the', 'poor', 'who', 'cannot', 'afford', 'a', 'plane', 'ticket', 'and', '/', 'or', 'order', 'things', 'via', 'next', '-', 'day', 'air', 'derive', 'very', 'little', 'benefit', 'from', 'the', 'faa', 'compared', 'to', 'a', 'person', 'who', 'owns', 'their', 'own', 'aircraft', 'and', 'can', 'fly', 'out', 'at', 'a', 'moments', 'notice', 'knowing', 'full', 'well', 'they', 'can', 'file', 'a', 'flight', 'plan', 'and', 'communicate', 'with', 'a', 'network', 'of', 'airports', 'to', 'ensure', 'their', 'plane', 'will', 'not', 'crash', 'into', 'any', 'other', 'jets', '.', '&', 'gt', ';', 'a', 'tank', ',', 'a', 'missile', ',', 'a', 'police', 'officer', 'protects', 'me', 'the', 'same', 'as', 'it', 'does', 'anyone', 'else', '.', 'a2', ':', 'but', ',', 'a', 'person', 'with', 'more', 'net', 'worth', 'has', 'more', 'to', 'lose', 'than', 'a', 'person', 'with', 'low', 'net', 'worth', '.', 'therefore', ',', 'even', 'independent', 'of', 'a1', 'above', ',', 'your', 'statement', 'is', 'false', '.', 'those', 'examples', 'protect', 'those', 'with', 'more', 'property', '/', 'net', '-', 'worth', '/', 'etc', 'more', '-', 'so', 'than', 'those', 'with', 'less', '.', '&', 'gt', ';', 'b', ')', 'as', 'a', 'percentage', 'of', 'income', ',', 'infrastructure', 'is', 'far', 'more', 'valuable', 'to', 'low', '-', 'income', 'individuals', 'than', 'high', '-', 'income', 'individuals', 'it', 'depends', 'on', 'the', 'infrastructure', ':', 'but', 'there', 'is', 'far', 'more', 'infrastructure', 'protecting', 'the', 'wealthy', 'than', 'the', 'poor', '.', 'your', 'example', 'is', 'the', 'stock', 'market', '.', 'why', 'should', 'the', 'vast', 'majority', 'of', 'people', 'pay', 'for', 'sec', 'and', 'rules', 'and', 'regulations', 'to', 'require', '/', 'enforce', 'honest', 'filing', '##s', 'when', 'they', 'cannot', 'afford', 'stock', '?', 'who', 'benefits', 'from', 'sec', 'infrastructure', '.', 'you', 'and', 'i', 'do', '.', 'value', 'to', 'poor', 'as', 'a', 'percentage', 'of', 'income', '=', '0', '%', '.', 'value', 'to', 'rich', '&', 'gt', ';', '0', '%', '.', 'q', '##ed', '.', 'your', 'roads', 'argument', 'as', 'an', 'example', 'of', 'poor', 'using', 'more', 'infrastructure', 'than', 'the', 'rich', 'is', 'a', 'bad', 'one', '.', 'the', 'poor', 'are', 'more', 'likely', 'to', 'take', 'public', 'transportation', 'and', '/', 'or', 'work', 'within', '5', 'miles', 'of', 'their', 'residence', '.', 'the', 'rich', 'are', 'more', 'likely', 'to', 'have', 'multiple', 'cars', ',', 'live', 'in', 'gate', '##d', 'areas', 'far', 'from', 'work', 'and', 'take', 'long', 'road', 'trips', '.', 'staying', 'at', 'home', 'to', 'work', 'is', 'a', 'function', 'of', 'more', 'than', 'just', 'owning', 'stock', '.', 'there', 'are', 'at', '-', 'home', '-', 'parents', ',', 'it', 'professionals', ',', 'programmers', ',', 'vo', '##ip', 'operators', ',', 'etc', ',', 'all', 'working', 'from', 'home', 'and', 'completely', 'independent', 'of', 'road', 'use', '.', '&', 'gt', ';', 'c', ')', 'the', 'activities', 'of', 'business', 'owners', 'generate', 'massive', 'tax', 'revenues', '.', 'these', 'far', 'out', '##weig', '##h', 'their', 'personal', 'utility', 'from', 'infrastructure', '.', 'c1', ':', '\"', 'personal', 'utility', '\"', 'you', 'are'])]\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.DataFrame(test_set)\n",
    "test_df = test_df.rename(columns={0: 'qid', 1: 'pos', 2:'cand'})\n",
    "# test_df = test_df[['qid', 'cand']]\n",
    "\n",
    "test_df.head(5)\n",
    "\n",
    "test_pos = test_df[['qid', 'pos']]\n",
    "test_pos = test_pos.explode('pos')\n",
    "test_pos = test_pos.rename(columns={'pos': 'docid'})\n",
    "test_pos['label'] = test_pos.apply(lambda x: 1, axis=1)\n",
    "\n",
    "len(test_pos)\n",
    "\n",
    "test_neg = test_df[['qid', 'cand']]\n",
    "test_neg = test_neg.explode('cand')\n",
    "test_neg = test_neg.rename(columns={'cand': 'docid'})\n",
    "test_neg ['label'] = test_neg .apply(lambda x: 0, axis=1)\n",
    "\n",
    "test_neg.head(5)\n",
    "\n",
    "test_data = pd.concat([test_pos, test_neg]).sort_values(by=['qid'])\n",
    "\n",
    "test_data['question'] = test_data['qid'].apply(lambda x: qid_to_text[x])\n",
    "test_data['ans_cand'] = test_data['docid'].apply(lambda x: label_to_ans[x])\n",
    "test_data['ques_token'] = test_data['question'].apply(lambda x: add_question_token(x))\n",
    "test_data['ans_cand'] = test_data['ans_cand'].apply(lambda x: add_ans_token(x))\n",
    "\n",
    "test_data = test_data[['qid', 'docid', 'label', 'ans_cand','ques_token']]\n",
    "test_data['seq'] = test_data['ques_token'] + test_data['ans_cand']\n",
    "test_data['seq_clipped'] = test_data['seq'].apply(clip)\n",
    "\n",
    "test_data.head(5)\n",
    "\n",
    "docid_map = test_data[['docid', 'seq_clipped']]\n",
    "test_docid_to_seq = {}\n",
    "\n",
    "for index, row in docid_map.iterrows():\n",
    "    test_docid_to_seq[row['docid']] = row['seq_clipped']\n",
    "\n",
    "print(take(5, test_docid_to_seq.items()))\n",
    "\n",
    "save_pickle(path+'data-bert/test_docid_to_seq_512.pickle', test_docid_to_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EewC_SM1fjqx"
   },
   "outputs": [],
   "source": [
    "# test_docid_to_seq = load_pickle(path+'data-bert/test_docid_to_seq.pickle')\n",
    "test_docid_to_seq = load_pickle(path+'data-bert/test_docid_to_seq_512.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vu7rnpVXPV3H"
   },
   "outputs": [],
   "source": [
    "def get_rank(model, test_set, qid_rel, max_seq_len):\n",
    "\n",
    "    qid_pred_rank = {}\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for i, seq in enumerate(tqdm(test_set)):\n",
    "        \n",
    "        qid, label, cands = seq[0], seq[1], seq[2]\n",
    "\n",
    "        cands_id = np.array(cands)\n",
    "\n",
    "        scores = []\n",
    "\n",
    "        for docid in cands:\n",
    "\n",
    "            seq_text = test_docid_to_seq[docid]\n",
    "\n",
    "            encoded_seq = tokenizer.convert_tokens_to_ids(seq_text)\n",
    "\n",
    "            input_ids = pad_seq(encoded_seq, max_seq_len)\n",
    "\n",
    "            att_mask = torch.tensor([[int(token_id > 0) for token_id in input_ids]]).to(device)\n",
    "            \n",
    "            input_ids = torch.tensor([input_ids]).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions\n",
    "                outputs = model(input_ids, token_type_ids=None, attention_mask=att_mask)\n",
    "\n",
    "            logits = outputs[0]\n",
    "\n",
    "            pred = torch.sigmoid(logits)\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            pred = pred.detach().cpu().numpy()\n",
    "\n",
    "            scores.append(pred[:,1][0])\n",
    "\n",
    "        # Get the indices of the sorted similarity scores\n",
    "        sorted_index = np.argsort(scores)[::-1]\n",
    "\n",
    "        # Get the docid from the sorted indices\n",
    "        ranked_ans = cands_id[sorted_index]\n",
    "\n",
    "        # Dict - key: qid, value: ranked list of docids\n",
    "        qid_pred_rank[qid] = ranked_ans\n",
    "\n",
    "    return qid_pred_rank\n",
    "    # MRR, average_ndcg, precision = evaluate(qid_pred_rank, qid_rel, k)\n",
    "\n",
    "    # return qid_pred_rank, MRR, average_ndcg, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oOR-OM-qbb3T"
   },
   "outputs": [],
   "source": [
    "toy_test_label = dict(itertools.islice(test_qid_rel.items(), 10))\n",
    "toy_test = test_set[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "xGa0KvT4cbbo",
    "outputId": "196a7bdd-1119-4abc-bb7a-eecc2679029e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|         | 1/10 [00:10<01:36, 10.67s/it]\u001b[A\n",
      " 20%|        | 2/10 [00:21<01:25, 10.67s/it]\u001b[A\n",
      " 30%|       | 3/10 [00:31<01:14, 10.67s/it]\u001b[A\n",
      " 40%|      | 4/10 [00:42<01:04, 10.67s/it]\u001b[A\n",
      " 50%|     | 5/10 [00:53<00:53, 10.67s/it]\u001b[A\n",
      " 60%|    | 6/10 [01:03<00:42, 10.67s/it]\u001b[A\n",
      " 70%|   | 7/10 [01:14<00:32, 10.67s/it]\u001b[A\n",
      " 80%|  | 8/10 [01:25<00:21, 10.67s/it]\u001b[A\n",
      " 90%| | 9/10 [01:36<00:10, 10.68s/it]\u001b[A\n",
      "100%|| 10/10 [01:46<00:00, 10.68s/it]\u001b[A\n",
      "\u001b[A"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(path+'model/2_model-bert-512.pt'))\n",
    "\n",
    "qid_pred_rank = get_rank(model, toy_test, toy_test_label, max_seq_len=512)\n",
    "# qid_pred_rank = get_rank(model, test_set_full, test_qid_rel, max_seq_len=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "colab_type": "code",
    "id": "UHjUlJszDAd7",
    "outputId": "72777fa4-c9f7-4432-eae8-3ae086ac9c09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Average nDCG@10 for 10 queries: 0.7\n",
      "\n",
      "MRR@10 for 10 queries: 0.01951951951951952\n",
      "\n",
      "Average Precision@1: 0.6\n"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "\n",
    "num_q = len(toy_test)\n",
    "\n",
    "MRR, average_ndcg, precision = evaluate(qid_pred_rank, test_qid_rel, k)\n",
    "\n",
    "print(\"\\n\\nAverage nDCG@{} for {} queries: {}\\n\".format(k, num_q, average_ndcg))\n",
    "\n",
    "print(\"MRR@{} for {} queries: {}\\n\".format(k, num_q, MRR))\n",
    "\n",
    "print(\"Average Precision@{}: {}\".format(1, precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RcfbXb5eBcX5"
   },
   "outputs": [],
   "source": [
    "save_pickle(path+'rank/2_bert_test_full.pickle', qid_pred_rank)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "bertQA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0d57a913dfdf4c81b5f512c1225987cb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14c13046daba400aab55cc5cf1be7bd9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1903567596324c4bb1d2eb457706dde9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "359a0a5f99e44691901bbd10ed97d52b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0d57a913dfdf4c81b5f512c1225987cb",
      "max": 440473133,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e98c5260bc4644a8bec4b7d1f44f0eff",
      "value": 440473133
     }
    },
    "3d763ae797f94699b8ee7a2c5b9c4243": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b254734589b742e0bcda7375fde020bc",
       "IPY_MODEL_80471ba96275477c9f38889af2ebee6e"
      ],
      "layout": "IPY_MODEL_959cad564e414167915a8c8b52119f65"
     }
    },
    "47833dff0af7450588de83821a007eb0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "590e0e137a5f40408336300cc1c2b530": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "68328bff023c4e12800897a62f26af4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "80471ba96275477c9f38889af2ebee6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_47833dff0af7450588de83821a007eb0",
      "placeholder": "",
      "style": "IPY_MODEL_68328bff023c4e12800897a62f26af4c",
      "value": "100% 232k/232k [00:00&lt;00:00, 420kB/s]"
     }
    },
    "894bb594ce08413eb0f865ec98e2035e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90171f1069794bb89292cf6b5c58d93d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cddf5a7330e84d4dbea9c4e84e03da2a",
      "placeholder": "",
      "style": "IPY_MODEL_de8f576fd9e94a59893a98763440e3bf",
      "value": "100% 361/361 [00:00&lt;00:00, 21.0kB/s]"
     }
    },
    "959cad564e414167915a8c8b52119f65": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9aba313875d046eba46b0f78345d13ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "aab583901fc64f2bb99347c4241ed340": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_df3ec5dcd9f34d6dabd4f4af50fd1c2c",
       "IPY_MODEL_90171f1069794bb89292cf6b5c58d93d"
      ],
      "layout": "IPY_MODEL_590e0e137a5f40408336300cc1c2b530"
     }
    },
    "b254734589b742e0bcda7375fde020bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d4a5e19bfd47438582b8c4d17137d85f",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1903567596324c4bb1d2eb457706dde9",
      "value": 231508
     }
    },
    "cddf5a7330e84d4dbea9c4e84e03da2a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d04fa886080e4a0c98dfe654a6c1a1a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_14c13046daba400aab55cc5cf1be7bd9",
      "placeholder": "",
      "style": "IPY_MODEL_fd34cfe552ab44ff8ea619a8474b1fa2",
      "value": "100% 440M/440M [00:35&lt;00:00, 12.4MB/s]"
     }
    },
    "d1c34f57b8ae42b496b2d20338415f42": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d4a5e19bfd47438582b8c4d17137d85f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de8f576fd9e94a59893a98763440e3bf": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "df3ec5dcd9f34d6dabd4f4af50fd1c2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_894bb594ce08413eb0f865ec98e2035e",
      "max": 361,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9aba313875d046eba46b0f78345d13ea",
      "value": 361
     }
    },
    "e98c5260bc4644a8bec4b7d1f44f0eff": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "f5d09a4ab8b84bffb37ac8d07317c877": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_359a0a5f99e44691901bbd10ed97d52b",
       "IPY_MODEL_d04fa886080e4a0c98dfe654a6c1a1a4"
      ],
      "layout": "IPY_MODEL_d1c34f57b8ae42b496b2d20338415f42"
     }
    },
    "fd34cfe552ab44ff8ea619a8474b1fa2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
